{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea8587da-4454-458e-94d3-d9fcf5db49ec",
   "metadata": {},
   "source": [
    "# VL05 - Language Modeling with N-grams\n",
    "In this seminar we explore the concepts behind language modeling and n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa7fc23-83a9-464d-b3ac-adf778ebef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b8ee3-8742-435b-957f-7c56c29cec9b",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset\n",
    "We will use the a sample of articles from wikipedia. In particular, the `Salesforce/wikitext` dataset from Huggingface. If you don't have access to internet from your notebook, use the script `download_dataset.py` and then `load_from_disk()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b34b64-c084-483b-8508-19b1d14343e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "train_texts = ds[\"train\"][\"text\"]  # list of strings\n",
    "eval_texts = ds[\"test\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15d62e2-6287-418c-891c-87da1c718625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataset\n",
    "df = ds[\"train\"].to_pandas()\n",
    "df.info()\n",
    "df[\"text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad5dedf-f84c-4ab7-96c6-553518db99a6",
   "metadata": {},
   "source": [
    "## 2. Building Bi-grams from a Corpus\n",
    "\n",
    "To estimate a bigram model, we first need to **extract word pairs** (bi-grams) from our text.  \n",
    "Each bigram represents two consecutive words, (w_{i-1}, w_i), which we’ll later turn into conditional probabilities (P(w_i | w_{i-1})).\n",
    "\n",
    "**Step 1: Clean the raw text**\n",
    "The Wikipedia articles in our dataset contain markup such as headings (`=== Section ===`), templates (`{{ ... }}`), or links (`[[link]]`).  \n",
    "We remove these artifacts to get plain text sentences suitable for tokenization.\n",
    "\n",
    "**Step 2: Tokenize and collect bigrams**\n",
    "For each cleaned line:\n",
    "1. Tokenize it using spaCy (or another tokenizer).  \n",
    "2. Add `<s>` and `</s>` markers to denote sentence boundaries.  \n",
    "3. Create consecutive word pairs and count them using Python’s `Counter`.\n",
    "\n",
    "The resulting counts (e.g., `Counter({(\"of\", \"the\"): 16949, (\"in\", \"the\"): 10560, ...})`)  \n",
    "form the basis for estimating probabilities and building a **bigram language model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0199f-1cf2-4085-83fa-823aa7d610cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def clean_wikitext(lines):\n",
    "    \"\"\"Remove obvious wikitext artifacts.\"\"\"\n",
    "    for t in lines:\n",
    "        t = t.strip()\n",
    "        if not t:\n",
    "            continue                          # skip blank lines (caused your <s>,</s> spike)\n",
    "        if re.match(r\"^=+\\s.*\\s=+$\", t):\n",
    "            continue                          # skip section headings like \"== History ==\"\n",
    "        # Strip templates/links (conservative to avoid nuking content)\n",
    "        t = re.sub(r\"\\{\\{.*?\\}\\}\", \"\", t)     # {{ ... }}\n",
    "        t = re.sub(r\"\\[\\[|\\]\\]\", \"\", t)       # [[link]] -> link\n",
    "        yield t\n",
    "\n",
    "def tokenize(s):\n",
    "    doc = nlp.make_doc(s) \n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def bigram_counts(texts):\n",
    "    B = Counter()\n",
    "    for raw in clean_wikitext(texts):\n",
    "        toks = tokenize(raw)\n",
    "        if not toks: continue\n",
    "            \n",
    "        toks = [\"<s>\"] + toks + [\"</s>\"]\n",
    "        B.update(zip(toks, toks[1:]))\n",
    "    return B\n",
    "\n",
    "# Example with WikiText\n",
    "B = bigram_counts(train_texts)\n",
    "B.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddef4c4-4b67-4a7a-bb24-b25a23245443",
   "metadata": {},
   "source": [
    "## 3. Next token proabilities\n",
    "\n",
    "To compute the probability of P(w_i | w_{i-1}) we need to compute:\n",
    "- count(w_{i-1}, w_i) : the total counts of each of the bigrams\n",
    "- count(w_{i-1}): the total count for the word w_{i-1} as a prefix\n",
    "\n",
    "We already have the total counts for the bigrams in the counter `B`. We can create another counter called `prefix_counts` to keep track of the counts for the prefix (w_{i-1}).\n",
    "\n",
    "### 3.1 Prob MLE\n",
    "We turn counts into probabilities with the Maximul likelihood estimate:\n",
    "$$P(w_i | w_{i-1}) = count(w_{i-1}, w_i) / count(w_{i-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8c3fa-60fe-4869-9a12-b4c72d28f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given: B = bigram_counts(train_texts) \n",
    "\n",
    "# prefix counts: C(w_{i-1}, *)\n",
    "prefix_counts = Counter()\n",
    "for (w1, w2), c in B.items():\n",
    "    prefix_counts[(w1,)] += c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caabf0c-c8e3-4d82-a89b-99a23ceeac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_mle(next_word, history, B, prefix_counts):\n",
    "    # history = (w_{i-1},)\n",
    "    c_bigram = B.get((history[0], next_word), 0)\n",
    "    c_prefix = prefix_counts.get(history, 0)\n",
    "    return (c_bigram / c_prefix) if c_prefix else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b1ea7-e597-4a65-8e3d-3657b9ff7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"C('of','the') =\", B.get(('of','the'), 0))\n",
    "print(\"C('of', *)   =\", prefix_counts.get(('of',), 0))\n",
    "\n",
    "prob_mle(\"the\", (\"of\",), B, prefix_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e3b37-77a5-417c-a73a-77607f2bd7bf",
   "metadata": {},
   "source": [
    "### 3.2 Prob Laplace\n",
    "One way to avoid 0 probabilities is Laplace smoothing. This is a case of Add-k smoothing, were alpha = 1.\n",
    "\n",
    "P(w_i | w_{i-1}) = ( count(w_{i-1}, w_i) + alpha) / ( count(w_{i-1}) + alpha * V_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc984d-93f9-446a-9c2c-ee18609e1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary from observed tokens in the bigrams\n",
    "def build_vocab_from_bigrams(B):\n",
    "    V = set()\n",
    "    for (w1, w2) in B.keys():\n",
    "        V.add(w1); V.add(w2)\n",
    "    return V\n",
    "\n",
    "VOCAB = build_vocab_from_bigrams(B)\n",
    "V_size = len(VOCAB)\n",
    "print (\"Vocabulary size: \", V_size)\n",
    "\n",
    "def prob_laplace(next_word, history, B, prefix_counts, V_size, alpha=1):\n",
    "    c_bigram = B.get((history[0], next_word), 0)\n",
    "    c_prefix = prefix_counts.get(history, 0)\n",
    "    return (c_bigram + alpha) / (c_prefix + alpha*V_size)\n",
    "\n",
    "print(\"prob_laplace[of the]: \", prob_laplace(\"the\", (\"of\",), B, prefix_counts, V_size))\n",
    "print(\"prob_mle: [of the]\", prob_mle(\"the\", (\"of\",), B, prefix_counts))\n",
    "\n",
    "print(\"prob_laplace[of Bielefeld]: \", prob_laplace(\"Bielefeld\", (\"of\",), B, prefix_counts, V_size))\n",
    "print(\"prob_mle: [of Bielefeld]\", prob_mle(\"Bielefeld\", (\"of\",), B, prefix_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f961972-1011-4c91-b16d-62b14756d119",
   "metadata": {},
   "source": [
    "### 3.3 Inspect TopK next tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca10b2f-1f09-4ddf-b340-b21b634a3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "# functools.partial lets you pre-fill some arguments of a function and get back a new, simpler function.\n",
    "#   instead of using lambda functions for prob_fn\n",
    "#   lambda w,h: prob_mle(w, h, B, prefix_counts)\n",
    "P_MLE     = partial(prob_mle,     B=B, prefix_counts=prefix_counts)\n",
    "P_LAPLACE = partial(prob_laplace, B=B, prefix_counts=prefix_counts, V_size=V_size, alpha=1.0)\n",
    "\n",
    "\n",
    "def topk_next(prev, prob_fn, k=10):\n",
    "    \"\"\"\n",
    "    Show the top-k most likely next words given a previous word.\n",
    "\n",
    "    prev:      the conditioning word (w_{i-1})\n",
    "    prob_fn:   a function returning P(next_word | history)\n",
    "    k:         number of results to display\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for (w1, w2) in B.keys():\n",
    "        if w1 == prev:\n",
    "            p = prob_fn(w2, (prev,))\n",
    "            if p > 0:\n",
    "                candidates.append((w2, p))\n",
    "\n",
    "    # sort by probability (highest first)\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return candidates[:k]\n",
    "\n",
    "\n",
    "print(\"MLE - Top after 'of':\", topk_next(\"the\", P_MLE))\n",
    "print(\"LAP - Top after 'of':\", topk_next(\"the\", P_LAPLACE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0cd91-ce40-41b9-aadc-13c601ebe973",
   "metadata": {},
   "source": [
    "## 4. Sentence Probabilities\n",
    "\n",
    "So far, we have probabilities for **individual word pairs** (bigrams).  \n",
    "To evaluate entire **sentences**, we combine these local probabilities using the **chain rule**:\n",
    "\n",
    "$$\n",
    "P(w_1, \\ldots, w_n) = \\prod_{i=1}^{n} P(w_i \\mid w_{i-1})\n",
    "$$\n",
    "\n",
    "In log space, this becomes a sum of log-probabilities — much more stable numerically.\n",
    "\n",
    "### What the functions do\n",
    "- **`sent_logprob`**  \n",
    "  Computes the total log-probability of a sentence and returns both:\n",
    "  - the sum of log probabilities (`lp`)\n",
    "  - the number of predicted tokens (`N`)\n",
    "\n",
    "- **`sent_prob`**  \n",
    "  Converts the log-probability back into the actual (tiny) probability value.  \n",
    "  Returns `0.0` for sentences containing unseen n-grams.\n",
    "\n",
    "These functions allow us to compare how **different models** (e.g., MLE vs. Laplace) assign probabilities to entire sentences rather than individual word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0833a-7bef-4146-a327-74f86bc5b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sent_logprob(tokens, prob_fn):\n",
    "    # predict every token except <s>\n",
    "    toks = [\"<s>\"] + tokens + [\"</s>\"]\n",
    "    N = len(toks) - 1\n",
    "    if N <= 0:\n",
    "        return -math.inf, 0\n",
    "    lp = 0.0\n",
    "    for i in range(1, len(toks)):\n",
    "        p = prob_fn(toks[i], (toks[i-1],))\n",
    "        if p == 0.0:\n",
    "            return -math.inf, N\n",
    "        lp += math.log(p)\n",
    "    return lp, N\n",
    "\n",
    "def sent_prob(tokens, prob_fn):\n",
    "    lp, _ = sent_logprob(tokens, prob_fn)\n",
    "    return 0.0 if lp == -math.inf else math.exp(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f925617-d16d-4132-af5c-6a7fbba2c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence:\n",
    "test_sentence = \"The force will be with me.\"\n",
    "\n",
    "# We used spacy earlier - we apply the same tokenisation here\n",
    "toks = tokenize(test_sentence)\n",
    "\n",
    "# MLE vs Laplace\n",
    "p_mle  = sent_prob(toks, P_MLE)\n",
    "p_lap  = sent_prob(toks, P_LAPLACE)\n",
    "lp_mle, _ = sent_logprob(toks, P_MLE)\n",
    "lp_lap, _ = sent_logprob(toks, P_LAPLACE)\n",
    "\n",
    "print(\"Sentence prob (MLE):     \", p_mle)\n",
    "print(\"Sentence prob (Laplace): \", p_lap)\n",
    "print(\"Log-prob (MLE):          \", lp_mle)\n",
    "print(\"Log-prob (Laplace):      \", lp_lap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441e2f1-c975-4b71-a0ea-92b225827a84",
   "metadata": {},
   "source": [
    "## 5. Next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e267d-d0db-4c03-bc8b-211792e48e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1) Index successors once: w1 -> list of (w2, count)\n",
    "successors = defaultdict(list)\n",
    "for (w1, w2), c in B.items():\n",
    "    successors[w1].append((w2, c))\n",
    "\n",
    "# 2) Unigram backoff if a word has no successors\n",
    "unigram_counts = {h[0]: c for h, c in prefix_counts.items()}\n",
    "\n",
    "def sample_next(prev):\n",
    "    \"\"\"\n",
    "    Sample next word from the empirical MLE P(. | prev).\n",
    "    If 'prev' has no observed successors, back off to unigram counts.\n",
    "    \"\"\"\n",
    "    pairs = successors.get(prev)\n",
    "    if pairs:  # use observed bigram counts as weights\n",
    "        words, weights = zip(*pairs)\n",
    "    else:      # back off to unigram counts\n",
    "        words, weights = zip(*unigram_counts.items())\n",
    "    return random.choices(words, weights=weights, k=1)[0]\n",
    "\n",
    "# Example: sample a few next words\n",
    "random.seed(0)\n",
    "for h in [\"of\", \"in\", \"<s>\"]:\n",
    "    print(h, \"→\", [sample_next(h) for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13468ae7-a2c2-4c64-aabe-338eb84bbea0",
   "metadata": {},
   "source": [
    "## 6. Sentence generation\n",
    "\n",
    "Once we can sample the next word $( w_i \\sim P(w_i \\mid w_{i-1}) )$,  \n",
    "we can generate full sentences by repeatedly predicting and sampling the next token.\n",
    "\n",
    "We start from the sentence-begin marker `<s>` and continue until either:\n",
    "- the model emits the end marker `</s>`, or  \n",
    "- we reach a maximum length.\n",
    "\n",
    "This produces sentences that reflect the statistical structure of the corpus —  \n",
    "frequent word pairs appear naturally, and the text roughly “sounds like” the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747298f-c6d5-47f3-a48e-fd65fc7d086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(max_len=25, start_token=\"<s>\", end_token=\"</s>\"):\n",
    "    \"\"\"Generate a random sentence using the bigram model.\"\"\"\n",
    "    prev = start_token\n",
    "    out = []\n",
    "    for _ in range(max_len):\n",
    "        nxt = sample_next(prev)\n",
    "        if nxt == end_token:\n",
    "            break\n",
    "        out.append(nxt)\n",
    "        prev = nxt\n",
    "    return out\n",
    "\n",
    "# Try it\n",
    "random.seed(7)\n",
    "print(\" \".join(generate_sentence()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af768c7f-e1bf-48a9-b97b-a26a4358f12a",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "We now evaluate our trained language models using **cross-entropy** and **perplexity** on held-out data.\n",
    "\n",
    "- **Cross-entropy (H):** measures the average “surprise” of the model on unseen text.  \n",
    "  Lower values mean better predictions.\n",
    "- **Perplexity (PP):** exp(H); indicates how many choices the model “considers possible” at each step.  \n",
    "  Lower PP = less uncertainty.\n",
    "\n",
    "To ensure fair evaluation:\n",
    "1. Use the same preprocessing as during training (tokenization, casing, padding).  \n",
    "2. Map unseen tokens to `<UNK>` so all models face the same vocabulary.  \n",
    "3. Compare models using the same metric on the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58eee78-06a4-4267-9ad3-18546ba56184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "\n",
    "def cross_entropy_perplexity(texts, prob_fn, tokenize_fn):\n",
    "    \"\"\"\n",
    "    Compute corpus cross-entropy (nats/token) and perplexity for a bigram model.\n",
    "\n",
    "    texts:       iterable of raw strings (sentences or lines)\n",
    "    prob_fn:     callable(next_word:str, history:tuple[str]) -> float\n",
    "                 (should return 0.0 for unseen events)\n",
    "    tokenize_fn: callable(text:str) -> list[str]\n",
    "    \"\"\"\n",
    "    total_logprob = 0.0\n",
    "    total_tokens  = 0\n",
    "    skipped_sents = 0\n",
    "\n",
    "    for text in texts:\n",
    "        toks = tokenize_fn(text)\n",
    "        # pad with sentence markers for bigrams: predict every token except <s>\n",
    "        toks = [\"<s>\"] + toks + [\"</s>\"]\n",
    "        if len(toks) < 2:\n",
    "            continue\n",
    "\n",
    "        sent_lp = 0.0\n",
    "        N = 0\n",
    "        zero_prob = False\n",
    "\n",
    "        for i in range(1, len(toks)):\n",
    "            nxt = toks[i]\n",
    "            hist = (toks[i-1],)          # bigram history\n",
    "            p = prob_fn(nxt, hist)\n",
    "            if p == 0.0:\n",
    "                zero_prob = True\n",
    "                break\n",
    "            sent_lp += math.log(p)\n",
    "            N += 1\n",
    "\n",
    "        if zero_prob:\n",
    "            skipped_sents += 1\n",
    "            continue\n",
    "\n",
    "        total_logprob += sent_lp\n",
    "        total_tokens  += N\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return math.inf, math.inf, skipped_sents\n",
    "\n",
    "    H  = - total_logprob / total_tokens     # cross-entropy (nats/token)\n",
    "    PP = math.exp(H)                        # perplexity\n",
    "    return H, PP, skipped_sents\n",
    "\n",
    "# define the partial functions for simplicity\n",
    "P_MLE     = partial(prob_mle,     B=B, prefix_counts=prefix_counts)\n",
    "P_LAPLACE = partial(prob_laplace, B=B, prefix_counts=prefix_counts, V_size=V_size, alpha=0.1)\n",
    "\n",
    "clean_eval = list(clean_wikitext(eval_texts))\n",
    "\n",
    "H_mle, PP_mle, skipped_mle = cross_entropy_perplexity(clean_eval, P_MLE, tokenize)\n",
    "H_lap, PP_lap, skipped_lap = cross_entropy_perplexity(clean_eval, P_LAPLACE, tokenize)\n",
    "\n",
    "print(f\"MLE      → H: {H_mle:.4f} nats | PP: {PP_mle:.2f} | skipped: {skipped_mle}\")\n",
    "print(f\"Lapl(0.1)→ H: {H_lap:.4f} nats | PP: {PP_lap:.2f} | skipped: {skipped_lap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4990d2-14c7-4baa-a9f9-594064a011c6",
   "metadata": {},
   "source": [
    "## 8. N-Grams with NLTK\n",
    "\n",
    "\n",
    "### 8.1 Preparing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0bca75-e68c-40a6-9e32-ff0dd1bd96a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from nltk.lm import Lidstone, MLE  # good default; try KneserNeyInterpolated too\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "# 1) Build a minimal pipeline: tokenizer + sentencizer (no tagger/ner/parser)\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")  # rule-based sentence boundaries\n",
    "\n",
    "# 2) Batched processing for speed\n",
    "def to_sents(lines, batch_size=1000):\n",
    "    sents = []\n",
    "    for doc in nlp.pipe((t for t in lines if t and t.strip()), batch_size=batch_size):\n",
    "        for s in doc.sents:\n",
    "            toks = [tok.text for tok in s if not tok.is_space]\n",
    "            if toks:\n",
    "                sents.append(toks)\n",
    "    return sents\n",
    "\n",
    "clean_text = clean_wikitext(train_texts) # function defined earlier\n",
    "sents_train = to_sents(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe72d8-0890-48d8-a5ac-63c4324869b0",
   "metadata": {},
   "source": [
    "### 8.2 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d191c7c2-702b-4b7a-afbf-09ffd8dd9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 3\n",
    "train_ngrams, train_vocab = padded_everygram_pipeline(n, sents_train)\n",
    "lm = MLE(n)\n",
    "#lm = Lidstone(0.1, n) # add-k smoothing, alpha=1 : laplace smoothing\n",
    "lm.fit(train_ngrams, train_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32f645-f6ff-43f8-afe1-d5bcc5f99242",
   "metadata": {},
   "source": [
    "### 8.3 Inspecting the score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42134c03-5ab3-4544-a28c-e49e7e597c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.score(\"the\", [\"end\", \"of\"])   # bigram example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff4fe5-400a-4756-b2ee-6db6f985933c",
   "metadata": {},
   "source": [
    "### 8.4 Text Generation with N-gram Models\n",
    "\n",
    "`lm.generate()` is NLTK’s built-in function for sampling text from a language model.  \n",
    "It repeatedly predicts the next token based on the model’s probability distribution:\n",
    "\n",
    "```python\n",
    "lm.generate(num_words, text_seed=[\"<s>\"])\n",
    "```\n",
    "\n",
    "However, this function is **very simple**:\n",
    "- It samples words **directly from the model’s learned probabilities**.\n",
    "- It **does not support** parameters like *temperature*, *top-k*.\n",
    "- You get the model’s “true” random generation — sometimes coherent, sometimes repetitive.\n",
    "\n",
    "We will see how to address this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55e9f0-9ccf-45a1-9df7-0c877113ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.generate(20, text_seed=[\"<s>\"])     # 20 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150758a3-2cf1-4d28-beb8-4ee95b29a5c4",
   "metadata": {},
   "source": [
    "## 9. Evaluating N-gram Language Models\n",
    "\n",
    "We now evaluate our trained language models using **cross-entropy** and **perplexity** on held-out data.\n",
    "\n",
    "- **Cross-entropy (H):** measures the average “surprise” of the model on unseen text.  \n",
    "  Lower values mean better predictions.\n",
    "- **Perplexity (PP):** exp(H); indicates how many choices the model “considers possible” at each step.  \n",
    "  Lower PP = less uncertainty.\n",
    "\n",
    "To ensure fair evaluation:\n",
    "1. Use the same preprocessing as during training (tokenization, casing, padding).  \n",
    "2. Map unseen tokens to `<UNK>` so all models face the same vocabulary.  \n",
    "3. Compare models using the same metric on the same test set.\n",
    "\n",
    "### 9.1 Training the Models\n",
    "\n",
    "We train multiple configurations to compare performance:\n",
    "\n",
    "- **MLE:** maximum likelihood estimate (no smoothing, zeros for unseen n-grams).  \n",
    "- **Lidstone(0.1):** additive smoothing with α=0.1 (assigns small probability to unseen n-grams).  \n",
    "- Both **bigrams (n=2)** and **trigrams (n=3)** are trained for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fc624-dacb-4ae9-8289-ecf840bf04d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def train_ngrams(sents_train, lm, n=2):\n",
    "    train_ngrams, train_vocab = padded_everygram_pipeline(n, sents_train)\n",
    "    lm.fit(train_ngrams, train_vocab)\n",
    "    return lm\n",
    "\n",
    "#lm = WittenBellInterpolated(n)           \n",
    "#lm = KneserNeyInterpolated(n)  \n",
    "\n",
    "# --- bigrams (n=2) ---\n",
    "lm2_mle   = train_ngrams(sents_train, MLE(2),            n=2)\n",
    "lm2_addk  = train_ngrams(sents_train, Lidstone(0.1, 2),  n=2)\n",
    "\n",
    "# --- trigrams (n=3) ---\n",
    "lm3_mle   = train_ngrams(sents_train, MLE(3),            n=3)\n",
    "lm3_addk  = train_ngrams(sents_train, Lidstone(0.1, 3),  n=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4411864f-95ff-459b-9dad-2001b8f648c8",
   "metadata": {},
   "source": [
    "### 9.2 Defining Evaluation Metrics\n",
    "\n",
    "We evaluate each model on a test set as before, using:\n",
    "\n",
    "- **Cross-entropy:** average negative log-probability per token.  \n",
    "- **Perplexity:** exp(H), lower is better.\n",
    "\n",
    "To make this fair:\n",
    "- We pad each sentence with the same `<s>` and `</s>` markers used in training.  \n",
    "- Unseen tokens are mapped to `<UNK>` to avoid zero probabilities.\n",
    "\n",
    "We redefine the functions to use the `nltk` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4ad7e-b416-4bb8-8a69-31477c9e19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sent_cross_entropy(lm, toks):\n",
    "    import math\n",
    "    n = lm.order\n",
    "    # pad like training: (n-1) <s>, and one </s> at the end\n",
    "    toks = ([\"<s>\"] * (n - 1)) + toks + [\"</s>\"]\n",
    "    lp = 0.0\n",
    "    for i in range(n - 1, len(toks)):\n",
    "        history = toks[i - (n - 1): i]       # last n-1 tokens\n",
    "        p = lm.score(toks[i], history)\n",
    "        if p == 0.0:\n",
    "            return math.inf\n",
    "        lp += math.log(p)\n",
    "    N = len(toks) - (n - 1)                  # predicted tokens\n",
    "    return -lp / N\n",
    "\n",
    "def map_oov(tokens, vocab):\n",
    "    return [w if w in vocab else vocab.unk_label for w in tokens]\n",
    "\n",
    "def corpus_perplexity(lm, sents):\n",
    "    Hs = []\n",
    "    for s in sents:\n",
    "        toks = map_oov(s, lm.vocab)          # map to <UNK> if needed\n",
    "        h = sent_cross_entropy(lm, toks)\n",
    "        Hs.append(h)\n",
    "        \n",
    "    finite = [h for h in Hs if math.isfinite(h)]\n",
    "    if not finite:\n",
    "        return math.inf, math.inf, len(Hs)\n",
    "    H = sum(finite) / len(finite)\n",
    "    PP = math.exp(H)\n",
    "    skipped = len(Hs) - len(finite)\n",
    "    return H, PP, skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91753c7-0f99-455c-af0e-38f665fb7d1b",
   "metadata": {},
   "source": [
    "### 9.3 Preparing the Evaluation Data\n",
    "\n",
    "We clean and tokenize the held-out corpus using the same preprocessing as training.\n",
    "This ensures that both train and test data follow the same format, vocabulary, and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d2f2e-83fe-4390-99e9-d717c2b74931",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = clean_wikitext(eval_texts) # function defined earlier\n",
    "sents_eval = to_sents(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8731fe-3d1c-43a5-9f7c-e99ec2e1b4e9",
   "metadata": {},
   "source": [
    "### 9.4 Comparing and Interpreting Results\n",
    "\n",
    "We compute cross-entropy, perplexity, and the number of skipped (infinite) sentences for each model.\n",
    "\n",
    "**How to interpret:**\n",
    "- Lower **cross-entropy** and **perplexity** → better predictive fit.  \n",
    "- Many **skipped** sentences → model assigns zero probability to unseen events (typical of MLE).  \n",
    "- Very high perplexity → model is over-smoothed or poorly calibrated.  \n",
    "- Trigrams often outperform bigrams (more context), but can overfit without smoothing.\n",
    "\n",
    "The table below summarizes the results for all configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb6077-8bd1-48f0-b433-f785ed177821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "models = OrderedDict({\n",
    "    \"Bigram MLE\":              lm2_mle,\n",
    "    \"Bigram Lidstone(0.1)\":    lm2_addk,\n",
    "    \"Trigram MLE\":             lm3_mle,\n",
    "    \"Trigram Lidstone(0.1)\":   lm3_addk,\n",
    "    # Add more if you like\n",
    "    #\"Trigram Lidstone (0.05)\":    train_ngrams(sents_train, Lidstone(0.05, 3), n=3),    \n",
    "    # \"Fourgram Lidstone (0.05)\":    train_ngrams(sents_train, Lidstone(0.05, 4), n=4),\n",
    "})\n",
    "\n",
    "rows = []\n",
    "for name, lm in models.items():\n",
    "    H, PP, skipped = corpus_perplexity(lm, sents_eval)\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"n\": lm.order,\n",
    "        \"Cross-Entropy (nats)\": H,\n",
    "        \"Perplexity\": PP,\n",
    "        \"Skipped\": skipped\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"Perplexity\", ascending=True)\n",
    "\n",
    "# Clean display\n",
    "fmt = {\n",
    "    \"Cross-Entropy (nats)\": \"{:.4f}\".format,\n",
    "    \"Perplexity\": \"{:.2f}\".format\n",
    "}\n",
    "display(df.style.format(fmt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b16f1-2f49-43d6-a511-d83b0d42b35a",
   "metadata": {},
   "source": [
    "### 9.5 Testing Text Generation\n",
    "\n",
    "We can compare the generated text after sampling next K words with different ngram models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3e1dd-4131-47bb-b5e4-2f7e07c3ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, re\n",
    "\n",
    "def detok(tokens):\n",
    "    s = \" \".join(tokens)\n",
    "    s = re.sub(r\"\\s+([.,;:!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"\\s+'\\s*s\\b\", \"'s\", s)\n",
    "    return s\n",
    "\n",
    "def gen_sentence(lm, max_len=25, seed=None, start_token=\"<s>\", end_token=\"</s>\"):\n",
    "    \"\"\"Generate a sentence using the model's native sampling (lm.generate).\"\"\"\n",
    "    n = lm.order\n",
    "    hist = [\"<s>\"] * (n - 1)\n",
    "    if seed:\n",
    "        hist += seed[-(n - 1):]  # use the TAIL of the seed\n",
    "\n",
    "    out = []\n",
    "    for _ in range(max_len):\n",
    "        nxt = lm.generate(1, text_seed=hist[-(n - 1):])\n",
    "        if nxt == end_token:\n",
    "            break\n",
    "        out.append(nxt)\n",
    "        hist.append(nxt)\n",
    "    return detok(out)\n",
    "\n",
    "# Compare models with the same seed & RNG for fairness\n",
    "random.seed(7)\n",
    "seed_tokens = [\"In\", \"the\"]\n",
    "for name, lm in models.items():\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(gen_sentence(lm, max_len=25, seed=seed_tokens), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58416b53-035e-44e2-a966-694de859fc69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
