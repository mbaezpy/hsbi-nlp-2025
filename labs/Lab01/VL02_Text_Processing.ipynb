{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77772d27-70d3-4cfb-8163-17ae9f8ff162",
   "metadata": {},
   "source": [
    "# VL02 - Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228e51a-d2be-43c0-bfeb-e0d2a04e069f",
   "metadata": {},
   "source": [
    "We'll use two different libraries, one that is good for education, and a more powerful one typically used in production. We'll contrast both through this lecture:\n",
    "\n",
    "1. **nltk**: Primarily used for teaching, research, and exploring algorithms in NLP, offering a huge collection of corpora and modular functions. Requires more effort and individual function calls (like sent_tokenize and separate downloads) as it gives you a wide range of options for each task.\n",
    "\n",
    "2. **spaCy**: Optimized for speed, efficiency, and production use, processing text much faster for real-world applications like chatbots or large-scale data analysis. Uses advanced pre-trained statistical models to process text in one go, automatically providing accurate sentence segmentation, Part-of-Speech tags, and Named Entity Recognition (NER).\n",
    "\n",
    "## 1. Loading the libraries\n",
    "Make sure to run the `install_env.sh` script to download additional dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68dbe33-68b0-4ac2-96fb-972811f39fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    nltk.download('punkt', quiet=True)\n",
    "except Exception:\n",
    "    nltk = None\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except Exception:\n",
    "    nlp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91eeda9-adf9-44e9-b4da-58dca9f364a6",
   "metadata": {},
   "source": [
    "## 2. Enconding and Unicode Normalisation\n",
    "\n",
    "### 2.1 Different encodings\n",
    "Unicode characters that look identical can be encoded in different ways. For example, the character √º may appear as:\n",
    "- a single precomposed code point √º (U+00FC), or\n",
    "- a decomposed sequence u (U+0075) + COMBINING DIAERESIS (U+0308).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2156a608-e5ae-49f4-9876-5e624034fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"M√§dchen\"== \"MaÃàdchen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cd3b3-2167-45a2-8d1b-8f01f9bfa1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'M√§dchen'       # 'M√§dchen' NFC\n",
    "s2 = 'Ma\\u0308dchen' # 'MaÃàdchen' NFD composed differently\n",
    "\n",
    "print(f's1 = {s1}, s2 = {s2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c484e6-3463-46fe-8e16-4090628f5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{s1} == {s2} ?', s1 == s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa560bce-7ce4-41ef-92f4-8ac0f0e9de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "s2 = unicodedata.normalize('NFC', s2)\n",
    "\n",
    "print(f'{s1} == {s2} ?', s1 == s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06323ff1-daf4-435f-bdbe-34ecea606b89",
   "metadata": {},
   "source": [
    "### 2.2 Encoding as an exploit\n",
    "\n",
    "#### 2.2.1 Full-Width characters\n",
    "Unicode includes \"Full-Width\" characters (used in East Asian typography), which look like standard ASCII characters but are treated as completely different characters by a computer.\n",
    "\n",
    "| Keyword | ASCII character | Full-width character | Unicode difference |\n",
    "|---:|:---:|:---:|:---|\n",
    "| F | `F` (U+0046) | `Ôº¶` | U+FF26 |\n",
    "| R | `R` (U+0052) | `Ôº≤` | U+FF32 |\n",
    "| E | `E` (U+0045) | `Ôº•` | U+FF25 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d30d36-8da0-4b99-ae81-679e49dac0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_standard = \"Claim your FREE prize now!\"\n",
    "message_exploit = \"Claim your Ôº¶Ôº≤Ôº•Ôº• prize now!\"\n",
    "\n",
    "def check_if_spam (message):\n",
    "    if( \"FREE\" in message): \n",
    "        print(\"SPAM:\\t\", message)\n",
    "    else: \n",
    "        print(\"HAM:\\t\", message)\n",
    "        \n",
    "check_if_spam(message_standard)\n",
    "check_if_spam(message_exploit)\n",
    "\n",
    "# Can we solve it with our normalisation?\n",
    "message_normalised = unicodedata.normalize('NFC', message_exploit)\n",
    "check_if_spam(message_normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b557a-3ffa-4310-a638-7c23128e650e",
   "metadata": {},
   "source": [
    "##### What do we do?\n",
    "\n",
    "In this situation, you might want to normalise to **NFKC**. NFKC normalizes characters that are compatibility variants of others (these are characters in Unicode intended to be typographic or compatibility forms). Examples NFKC will change:\n",
    "- full-width latin letters Ôº¶ ‚Üí F\n",
    "- ligatures Ô¨Å ‚Üí fi\n",
    "- compatibility symbols „éè ‚Üí kg\n",
    "- circled numbers ‚ë† ‚Üí 1\n",
    "- superscripts ¬≤ ‚Üí 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42940d-582f-4843-af77-bdeb1935795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples to run\n",
    "samples = {\n",
    "    \"fullwidth\": \"Ôº®ÔΩÖÔΩåÔΩåÔΩè ÔºëÔºíÔºì\",       # fullwidth Latin + fullwidth digits\n",
    "    \"ligature\": \"office Ô¨Åle\",             # 'Ô¨Å' ligature inside a word\n",
    "    \"compat_kg\": \"ÈáçÈáè: „éè\",               # U+338F SQUARE KG -> 'kg'\n",
    "    \"superscript\": \"x¬≤ + y¬≥\",             # superscripts -> digits\n",
    "    \"circled\": \"‚ë† ‚ë° ‚ë¢\",                  # circled numbers -> digits\n",
    "    \"angstrom_sign\": \"\\u212B\",            # ANGSTROM SIGN (compat) -> '√Ö'\n",
    "    \"umlaut\" : 'Ma\\u0308dchen',\n",
    "}\n",
    "\n",
    "def show(s):\n",
    "    print(\"ORIG     :\", s, \" ->\", [f\"U+{ord(ch):04X}\" for ch in s])\n",
    "    nfc = unicodedata.normalize(\"NFC\", s)\n",
    "    nfkc = unicodedata.normalize(\"NFKC\", s)\n",
    "    print(\"NFC      :\", nfc, \" ->\", [f\"U+{ord(ch):04X}\" for ch in nfc])\n",
    "    print(\"NFKC     :\", nfkc, \" ->\", [f\"U+{ord(ch):04X}\" for ch in nfkc])\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "for name, s in samples.items():\n",
    "    print(\"SAMPLE:\", name)\n",
    "    show(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5308492-5fda-48be-b2b3-e4cb94d61736",
   "metadata": {},
   "source": [
    "##### Defending against encoding exploits\n",
    "When encoding attacks might be needed, or if we want to normalise for better search and matching, we can use NFKC. \n",
    "Does the code below defend agaisnt the attack?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8391f46-1636-45fc-b66a-f4d46f87a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_normalised = unicodedata.normalize('NFKC', message_exploit)\n",
    "check_if_spam(message_normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4d3a8-62af-4f91-9c75-0922cbcf16ec",
   "metadata": {},
   "source": [
    "#### 2.2.2 Zero-width exploits\n",
    "Compatibility normalization (`NFKC`) is very useful (full-width ‚Üí ASCII, ligatures ‚Üí constituent letters, etc.), but it does **not** remove invisible / zero-width characters such as ZWSP, ZWNJ, ZWJ or the BOM.  \n",
    "These characters can appear accidentally (copy/paste, editors) and they will break literal substring/regex rules unless you remove or canonicalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55693e9-d24c-4845-b8c6-475cdc72bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Claim your F‚ÄåREE prize now!\"\n",
    "check_if_spam(message)\n",
    "\n",
    "message_normalised = unicodedata.normalize('NFKC', message)\n",
    "check_if_spam(message_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eff7e9-1d6f-4732-bbc0-6324af957f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e05363-8ba9-4154-8717-e069b35fbe9e",
   "metadata": {},
   "source": [
    "We can see above that we sneaked in an invisible character. We can deal with them by simply removing them. Characters and their unicode codes:\n",
    "\n",
    "```\n",
    "  ZWSP    ZWNJ     ZWJ    BOM   \n",
    " \\u200B  \\u200C  \\u200D  \\uFEFF   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c32ed9-820e-4e21-9f29-0576d941b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_zero_re = re.compile(\"[\\u200B\\u200C\\u200D\\uFEFF]\")\n",
    "\n",
    "message_clean = _zero_re.sub('', message)\n",
    "check_if_spam(message_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ba4d4-8d7d-4470-bfa1-f47cd4eb7a76",
   "metadata": {},
   "source": [
    "## 3. Sentence segmentation\n",
    "\n",
    "### 3.1. Using regular expressions\n",
    "We can use regular expressions to split a text into sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e426c1-8586-4e2d-adf1-fe1c0b61aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_short = \"You have won a prize! Claim your gift now.\"\n",
    "\n",
    "pattern = r'(?<=[.!?])\\s+(?=[A-Z0-9\"‚Äú\\'\\(\\[])'   # split after .!? when next char looks like sentence start\n",
    "\n",
    "sentences_short = re.split(pattern, text_short)\n",
    "\n",
    "# Print sentences\n",
    "def print_sentences (sentences):\n",
    "    for i,s in enumerate(sentences,1):\n",
    "        print(i, repr(s))\n",
    "\n",
    "print_sentences(sentences_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8153094d-7e6e-477a-baae-b0096361bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_long = (\n",
    "    \"Congratulations! You have won $1,000.00. Contact Dr. O'Neil at 9:00 a.m. to claim your prize. \"\n",
    "    \"Offer valid for U.S. residents only. See Sec. 3.2 for terms... Don't miss out! Visit www.example.com/free-offer \"\n",
    "    \"or call 1-800-555-0199. Mr. Smith, CEO of Acme Ltd., says, \\\"Act now!\\\"\"\n",
    ")\n",
    "\n",
    "sentences = re.split(pattern, text_long)\n",
    "print_sentences(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d1609-a607-4483-b3c5-b779e2380f2d",
   "metadata": {},
   "source": [
    "### 3.2 Using `ntlk` sentence segmentation\n",
    "NLTK's `sent_tokenize` uses the Punkt sentence tokenizer ‚Äî a data-driven model that detects sentence boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f3d29-f90f-4b20-9649-92b9886e8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_long = sent_tokenize(text_long, language='english')\n",
    "print_sentences(sentences_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec13a05-63a9-48de-9625-86e988c2d5ed",
   "metadata": {},
   "source": [
    "### 3.3 Using `spacy` pipeline\n",
    "spaCy performs sentence segmentation inside the pipeline via sentencizer/parser component. You access the results via `doc.sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6772b4-c2cd-4f98-87ef-93cd74a95ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text_long)\n",
    "\n",
    "# Get sentences from the Doc object\n",
    "sentences_long_spacy = [sent.text.strip() for sent in doc.sents]\n",
    "print_sentences(sentences_long_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf7142f-3ddd-4f0d-8eeb-afbfc54ebe7d",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "### 4.1 Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04d147-d7d4-4a3a-ab8e-deaa7f5bfd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'You have won a prize!'\n",
    "sent2 = \"Don't miss this opportunity\"\n",
    "\n",
    "print ( re.findall(r\"\\b\\w[\\w'\\-]*\\b\", sent1) )\n",
    "print ( re.findall(r\"\\b\\w[\\w'\\-]*\\b\", sent2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d03b91-32da-4d0b-a45b-4c85f74b38b5",
   "metadata": {},
   "source": [
    "### 4.2 Using `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c17cce-0d8f-4c1e-8498-603b3aac258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print ( word_tokenize(sent1) )\n",
    "print ( word_tokenize(sent2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a8bc1-03e1-43cc-a5e8-bbe9675e147c",
   "metadata": {},
   "source": [
    "### 4.3 Using `spacy`\n",
    "The doc acts as a sequence of token objects, and you iterate on it to have access to all the tokens from a document. To work at the level of sentence you should access them through `doc.sents`, which is a slice of the doc tokens, for that sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e2d43-3eaf-443a-84b7-ac19f1bae077",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent1)\n",
    "doc2 = nlp(sent2)\n",
    "print ( [t.text for t in doc1] ) ## all tokens\n",
    "print ( [t.text for t in doc2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1fd3a6-2126-49ba-b462-bcc3c00b40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"You have won a prize! Claim your gift now.\") \n",
    "for i, sent in enumerate(doc.sents, 1):\n",
    "    print(f\"Sentence {i}: {sent.text}\")\n",
    "    # tokens inside the sentence:\n",
    "    for token in sent:\n",
    "        print(\"   \", \"is_start:\", token.is_sent_start, \"\\t\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d2283-b62f-4d51-bc30-f8b3d5ed336a",
   "metadata": {},
   "source": [
    "### 4.4 Challenges with German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a039ca-ab61-4d3c-a82b-cabe608ddded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Challenge: A standard tokenizer (like a simple regex or a basic word splitter)\n",
    "# will treat the entire compound word as one token.\n",
    "\n",
    "sent_german = \"Der Krankenhaushaftpflichtversicherungsvertrag ist unklar.\"\n",
    "\n",
    "# Simulate simple tokenization (e.g., splitting by space)\n",
    "re.findall(r\"\\b\\w[\\w'\\-]*\\b\", sent_german)\n",
    "\n",
    "# Output (Error):\n",
    "# ['Der', 'Krankenhaushaftpflichtversicherungsvertrag', 'ist', 'unklar.']\n",
    "\n",
    "# The 'Solution' requires specialized tools (like spaCy's morphology component) \n",
    "# to split the word internally for proper analysis.\n",
    "\n",
    "# Expected Correct Tokens for the compound word:\n",
    "# [\"Krankenhaus\", \"Haftpflicht\", \"Versicherung\", \"Vertrag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2fe91-f63f-4c1c-bc2a-06f97ba9dbed",
   "metadata": {},
   "source": [
    "Run `python -m spacy download de_core_news_sm` before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eff126-befa-43e7-9c3a-2e355c6597e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# 2. Process the text\n",
    "doc_de = nlp_de(sent_german)\n",
    "\n",
    "# 3. Extract tokens\n",
    "[token.text for token in doc_de]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738291fa-5fee-4b04-9bc7-72a1402f50fe",
   "metadata": {},
   "source": [
    "Tokenization is separate from morphological analysis. Long German compounds (e.g. Krankenhaushaftpflichtversicherungsvertrag) are tokenized by spaCy as a single token ‚Äî splitting them into meaningful parts (decompounding) requires extra processing.\n",
    "For a fast, ready-to-run approach you can use CharSplit (compound-split):\n",
    "\n",
    "``pip install compound-split``\n",
    "\n",
    "`compound-split` returns ranked binary split candidates for a compound.\n",
    "- The top candidate is the model‚Äôs preferred split (left + right).\n",
    "- The output includes a numeric score; higher = more confident.\n",
    "- Scores can be negative for rare/awkward splits ‚Äî treat them as less confident.\n",
    "\n",
    "You can recursively apply the splitter to split multi-part compounds into smaller parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd645ac-35c6-434f-8e7a-69c589d79d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compound_split import char_split   # module exported by package\n",
    "\n",
    "words = [\n",
    "    \"Krankenhaushaftpflichtversicherungsvertrag\",\n",
    "    \"Krankenversicherung\",\n",
    "    \"Autobahnrastst√§tte\",\n",
    "    \"Kindergartenfreundschaft\",\n",
    "]\n",
    "\n",
    "for w in words:\n",
    "    splits = char_split.split_compound(w)   # returns ranked candidate (binary) splits\n",
    "    print(\"WORD:\", w)\n",
    "    for score, left, right in splits[:3]:   # show top 3 candidates\n",
    "        print(f\"  score={score:.3f} -> {left} + {right}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c0a26-9c07-4f92-8033-8cb2eefec6d0",
   "metadata": {},
   "source": [
    "## 5. Case folding, punctuation, emoji handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f62bec-33ef-460b-b210-a95d7deb7593",
   "metadata": {},
   "source": [
    "### 5.1 Case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fb418-99f6-4b57-8d6c-75718b273f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['WIN', 'Win', 'win']\n",
    "[t.lower() for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a853c-a886-4543-ac76-c5efeac9bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_de = ['Stra√üe', 'STRASSE', \"Osnabr√ºck\", \"OSNABRUECK\"]\n",
    "t_lowered = [t.lower() for t in tokens_de]\n",
    "t_folded = [t.casefold() for t in tokens_de]\n",
    "\n",
    "print(t_lowered)\n",
    "print(t_folded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace5866-5441-47c2-a120-e5a026d88dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(s):\n",
    "    return ''.join(ch for ch in unicodedata.normalize('NFKD', s) # decopo\n",
    "                   if not unicodedata.combining(ch))\n",
    "remove_diacritics(\"osnabr√ºck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c103f-a53a-4972-bd07-ae9435436f84",
   "metadata": {},
   "source": [
    "### 5.2 Dealing with \"special\" characters\n",
    "German (and many other European languages) use characters that are not ASCII ‚Äî e.g. umlauts (√§/√∂/√º), the sharp-S (√ü) and other diacritics (√©, √±, ‚Ä¶). How you handle them depends on the task. Below are the usual options and a short guideline.\n",
    "\n",
    "- **Keep them (do nothing)**. Keep the original characters. This preserves all linguistic information and is recommended for most modern ML models and linguistic analyses.\n",
    "- **Casefold only (caseless matching)**. Use Unicode case-folding (.casefold()) to compare case-insensitively. Important: .casefold() also maps √ü ‚Üí ss, unlike .lower().\n",
    "- **Strip diacritics / ASCII transliteration**. Convert accented letters to closest ASCII equivalents (e.g., √º ‚Üí u) using Unicode decomposition or a library (Unidecode, ICU). This loses diacritic information but can increase recall in legacy systems or ASCII-only contexts.\n",
    "- **Orthographic mapping (German-style)**. Apply a language-aware mapping such as √§ ‚Üí ae, √∂ ‚Üí oe, √º ‚Üí ue, √ü ‚Üí ss. This preserves more of the original spelling conventions (useful for legacy matching, usernames, domain names, or keyboard variants).\n",
    "\n",
    "What we choose depends on the type of task. When retreival tasks (matching, search) we probably want to strip it, for ML we probably don't want to strip  them, as we benefit from nuance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4d0d3-7d50-4c4d-a053-0c20a1059c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing\n",
    "text = \"üéâ WIN a FREE vacation NOW!! üèñÔ∏è\"\n",
    "\n",
    "# we could remove ! from the list if we want to retain it\n",
    "text_clean = re.sub(r\"[\\.,;:\\\"\\(\\)\\[\\]\\\\/\\?@#\\!\\$%\\^&\\*_+=<>~`|]+\", ' ', text) \n",
    "print(text, \" -> \", text_clean)\n",
    "\n",
    "# Notice that you would typically just filter the tokens (e.g., token.pos_ == PUNCT)\n",
    "text = \"üéâ WIN a FREE vacation... NOW!! üèñÔ∏è\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(\"   \", token.text, \" -> \" , token.pos_, )\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f92c76-d488-4e2b-a9ff-e0363bd7339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "text = emoji.replace_emoji(text, replace=' <EMOJI> ')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c894e46f-6c6b-4d8f-acc7-d390dfa268b4",
   "metadata": {},
   "source": [
    "### 5.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc75f5-b1cf-40d8-b79f-4046003dc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except Exception:\n",
    "    nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d744b-dcbc-472b-a786-3ab4e18aaf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# nltk\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [ t for t in tokens if t not in STOPWORDS] \n",
    "\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a38fa6-d2a2-430c-b6fa-10f0898a2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "    \n",
    "filtered_tokens_spacy = [token.text for token in doc \n",
    "                         if not token.is_stop and token.is_alpha]\n",
    "\n",
    "print(filtered_tokens_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d636f-55db-4f38-9320-76523721e675",
   "metadata": {},
   "source": [
    "## 6. Stemming and Lematization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0773e2c-8799-47a5-aa54-30f6bac82c68",
   "metadata": {},
   "source": [
    "### 6.1 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d081b-3cce-4beb-9dc7-91812912799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball_en = SnowballStemmer(\"english\")\n",
    "snowball_de = SnowballStemmer(\"german\")\n",
    "\n",
    "print (porter.stem('study') )\n",
    "print (snowball_en.stem('study') )\n",
    "print (snowball_de.stem('Krankenh√§user') ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8922753-2b91-415b-9c2a-499d10840604",
   "metadata": {},
   "source": [
    "Comparing stems and lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf58b9-4a40-4e7c-bf9a-0bbc9f577e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_en = ['study', 'studies', 'studying', 'studied', 'run', 'running', 'ran', 'better', 'was', 'educate']\n",
    "words_de = ['gehen', 'ging', 'gegangen', 'Krankenhaus', 'Krankenh√§user', 'arbeiten', 'arbeitete']\n",
    "\n",
    "def compare_stems(words, lang='en'):\n",
    "    print (\"Language : \", lang)\n",
    "    for w in words:\n",
    "        p = porter.stem(w) \n",
    "        s_en = snowball_en.stem(w) \n",
    "        s_de = snowball_de.stem(w) \n",
    "        \n",
    "        print(f\" [{lang}] {w:15} | porter: {p:10} | snow_en: {s_en:10} | snow_de: {s_de:10}\")\n",
    "        \n",
    "compare_stems(words_en, \"en\")  \n",
    "compare_stems(words_de, \"de\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03040ff0-8f9a-4fe6-a913-703c0ef0067b",
   "metadata": {},
   "source": [
    "### 6.2 Lemmatization\n",
    "\n",
    "#### Using ntlk\n",
    "We need to do some work when performing lemmatization with ntlk. We can clearly see the need for POS before lemmatization in\n",
    "`wln.lemmatize(token, pos)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93ac9d-7272-4d65-96e7-00df9020b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def treebank_to_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map NLTK/Treebank POS tags to WordNet POS tags for lemmatizer.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # sensible default\n",
    "\n",
    "def lemmatize_sentence(sent: str):\n",
    "    tokens = word_tokenize(sent)\n",
    "    tags = pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for token, tag in tags:\n",
    "        wn_pos = treebank_to_wordnet_pos(tag)\n",
    "        lemma = wnl.lemmatize(token, pos=wn_pos)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# Example\n",
    "print(lemmatize_sentence(\"You deserve better rewards for your loyalty.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdcdcb-052b-4408-ba5f-316ac2f0db25",
   "metadata": {},
   "source": [
    "#### Using spacy\n",
    "The lemmatizer is already in spacy's pipeline, along with the POS. We can simply access the lemma with `token.lemma_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7db77a-386f-40c1-8bf7-fb056a55e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To facilitate comparision, let's take the same words we used before\n",
    "# we make a \"sentence\" out of it, since spacy input is the sentence\n",
    "doc1 = nlp (\"You deserve better rewards for your loyalty\")\n",
    "\n",
    "print ([token.lemma_ for token in doc1])\n",
    "\n",
    "doc2 = nlp (\"Get good benefits now - exclusively for loyal members!\")\n",
    "\n",
    "print ([token.lemma_ for token in doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b431347a-e9ce-4ec4-8960-80e6be758e87",
   "metadata": {},
   "source": [
    "#### Inspect the POS and lemma\n",
    "You can insect the POS and the lemma below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679b9e6-9f79-492f-9db4-27be94db8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(\"{:<10} {:<10} {:<10}\".format(\n",
    "        token.text,             \n",
    "        token.pos_,      # Coarse-grained POS tag (e.g., NOUN, VERB)\n",
    "        token.lemma_,   \n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5ea4e-deef-49b7-980d-dbf4a03ab5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from IPython.display import HTML, display # Manually import from the standard location\n",
    "# Get the raw HTML string\n",
    "\n",
    "def display_parse_tree(doc):\n",
    "    html_code = displacy.render(doc, style='dep', jupyter=False)\n",
    "    \n",
    "    # Use the standard IPython display to show the HTML\n",
    "    display(HTML(html_code))\n",
    "    \n",
    "display_parse_tree(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02490a4-1a8d-4c64-b277-6e7b07ff2af3",
   "metadata": {},
   "source": [
    "## 7. A pre-processing pipeline\n",
    "\n",
    "### 7.1 Removing punctuation before segementing sentences and tokens\n",
    "We typically perform first segmentation and tokenization before removing punctuation. Otherwise this could affect the segmentation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0093e961-5cfa-4f23-bc71-d239965e2e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation (text):\n",
    "    return re.sub(r\"[\\.,;:\\\"\\\\'(\\)\\[\\]\\\\/\\?@#\\!\\$%\\^&\\*_+=<>~`|]+\", '', text) \n",
    "\n",
    "def tokenize (text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "text = \"Don't click here!\"\n",
    "\n",
    "text_clean = remove_punctuation(text);\n",
    "tokenize(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20ee61-57d3-4080-8e2f-4c107d964ed1",
   "metadata": {},
   "source": [
    "### 7.2 Issues with Casefolding before lemmatisation\n",
    "If you lowercase first, it can change the POS tagging.\n",
    "\n",
    "\"Reading is a lovely town.\"\n",
    "\"They are reading the book.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c9c4d-4000-4e28-9a70-00e1ffedd3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"May is a great month.\"\n",
    "text2 = \"I may go later.\"\n",
    "\n",
    "#doc1 = nlp(text1.casefold())    \n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)\n",
    "\n",
    "display_parse_tree(doc1)\n",
    "display_parse_tree(doc2)\n",
    "\n",
    "print(\"doc1\", get_lemma(doc1))\n",
    "print(\"doc2\", get_lemma(doc2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c200a-45ee-4072-8951-0776053b3077",
   "metadata": {},
   "source": [
    "### 7.3 Issues with stopword removal before lemmatisation\n",
    "If we remove stopwords before lemmatization, this can change the structure of the sentence (leading to bad POS). Stopwords are typi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9366e2-37fd-4108-8aac-0b376fff68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small demonstration of stopword-matching pitfalls\n",
    "text = \"The man wasn't running to the store\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# typical lowercased stoplist\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "print (tokens)\n",
    "# (A) Removing stopwords before lemmatization/case-normalization\n",
    "#     We manually force to do it in the incorrect order\n",
    "removed_before = [t for t in tokens if t.lower() not in stoplist]\n",
    "print(\"Tokens A - remaining after stopword removal - (not recommended):\", removed_before)   # 'He' removed? depends on .lower() use\n",
    "\n",
    "# (B) Lemmatize first (using spaCy), then remove with lowercased stoplist\n",
    "doc = nlp(text)\n",
    "lemmas = [tok.lemma_ for tok in doc]\n",
    "filtered_after = [tok for tok in lemmas if tok not in stoplist]\n",
    "print(\"Tokens B - remaining after stopword removal - (recommended):\", filtered_after)\n",
    "print(\"->lemmas:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a2f0e-07bf-4a18-ab8b-cc0f990d41b0",
   "metadata": {},
   "source": [
    "## 8. Practical notes about `spacy`  pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b179c07-96c2-48ef-a42b-e7bfde5ef2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.pipeline)\n",
    "print(nlp.pipe_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72910ce2-ecad-4560-ad3c-a23c10d2bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER\n",
    "text = \"Maria was walking in Paris. That was far from the United States of America\"\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# Running just the tokenizer\n",
    "doc_tokens_only = nlp.make_doc(text)\n",
    "print(\"Tokens (make_doc):\", [t.text for t in doc_tokens_only])\n",
    "print(\"Lema  (make_doc):\", [t.lemma_ for t in doc_tokens_only])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd290f7a-2dc4-4569-b9cb-de9b2232ffb6",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c710bc5-02b5-44e7-bf9e-9e9383417c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b010b7-0f0c-4a97-b022-10f4f7439955",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://spacy.io/usage/processing-pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
