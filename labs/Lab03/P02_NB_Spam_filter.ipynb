{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d8b739-635f-4d1a-b890-ce619ef3a83a",
   "metadata": {},
   "source": [
    "# Praktikum 2: Naive Bayes Spam Classifier\n",
    "\n",
    "In this lab, we extend the Naïve Bayes text classification workflow introduced in the seminar by applying it to a larger, real-world dataset and exploring model behavior, feature design, and evaluation in depth.\n",
    "\n",
    "**Dataset**:\n",
    "telegram-spam-ham (Hugging Face: https://huggingface.co/datasets/thehamkercat/telegram-spam-ham).\n",
    "Contains text messages labeled as spam or not_spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42c4c5-05f4-4ba2-a851-6f579c10392e",
   "metadata": {},
   "source": [
    "## Loading libraries\n",
    "In the repository, you will find the scripts to install the dependencies required for this lab under the `/scripts` folder. \n",
    "- If you are in a YourAI cluster node, make sure to run the `install_env.sh` script to download additional dependencies. \n",
    "- If you are in `google collabs`, open the terminal and load the commands in the `install_google_collabs.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152009d2-7870-4cac-86cd-b44cff271dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "except OSError:\n",
    "    print(\"Warning: spaCy model 'en_core_web_sm' not found. Please run 'python -m spacy download en_core_web_sm'\")\n",
    "    # Fallback to a simpler model creation if the standard one fails\n",
    "    nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4aa2f2-9b8c-4189-90b3-858540ece100",
   "metadata": {},
   "source": [
    "## Task 1. Load dataset\n",
    "\n",
    "**Goal**: familiarize yourself with the dataset and ensure it’s ready for training.\n",
    "\n",
    "**Steps**: \n",
    "\n",
    "1. Load the train/test splits.\n",
    "2. Inspect class distribution, number of documents, and average message length.\n",
    "3. Clean duplicates and missing values if any.\n",
    "\n",
    "\n",
    "**Deliverables**:\n",
    "- Short summary of dataset stats (counts, class ratio, sample messages).\n",
    "- Changes to the code, to clean and prepare the data\n",
    "\n",
    "**Note**: \n",
    "Make sure you run:\n",
    "\n",
    "```sh\n",
    "python scripts/download_dataset.py \"thehamkercat/telegram-spam-ham\" data/telegram-spam-ham\n",
    "```\n",
    "\n",
    "at the repository root.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28b2e5-5b3a-45ef-a430-f847c7048734",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"../../data/telegram-spam-ham\")\n",
    "df_telegram_all =  ds[\"train\"].to_pandas()\n",
    "df_telegram_all.columns = [\"label\", \"text\"]\n",
    "\n",
    "# dataset is too big\n",
    "df_telegram, _ = train_test_split(\n",
    "    df_telegram_all, train_size=10_000, stratify=df_telegram_all[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# create split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_telegram[\"text\"], df_telegram[\"label\"], test_size=0.2, random_state=42, stratify=df_telegram[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9643b-77c7-4252-a88a-2f87597b103e",
   "metadata": {},
   "source": [
    "## Task 2. Train a baseline NB spam classifier with basic pre-processing\n",
    "\n",
    "**Goal**: train a baseline MultinomialNB classifier using a Bag-of-Words representation, reflecting on its performance.\n",
    "\n",
    "**Steps**: \n",
    "1. Record the baseline performance\n",
    "2. Inspect the top 300 features/words with higher learned probaility `P(c|w)`. Use the provided utility function `preview_errors_explained()`.\n",
    "3. Reflect on the main causes for the current misclassifications\n",
    "\n",
    "**Deliverable**: \n",
    "- Write down your interpretation of the limitations of this baseline implementation\n",
    "- Write down a list of potential pre-processing operations you would implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28571b-bf56-4039-9e7b-c882f01f0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_word_count = 1 \n",
    "vectorizer = CountVectorizer(stop_words='english', \n",
    "                             min_df=min_word_count) # drop words that appear in less than 'min_word_count' document\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(X_train) # fit to the training split\n",
    "X_test_bow  = vectorizer.transform(X_test)      # transform the test split\n",
    "\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test_bow)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix.\")\n",
    "cm = confusion_matrix(y_test, y_pred, labels=nb.classes_) \n",
    "print(cm)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb.classes_)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(f\"Confusion Matrix — Naïve Bayes (Simple)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825744f-0398-4c2c-9a44-709b0571d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function!\n",
    "def print_indicative_features(nb, vectorizer, topk=10, verbose=True):\n",
    "    # 1. Identify class indices (spam = 1, ham = 0)\n",
    "    classes = nb.classes_.tolist()\n",
    "    i_spam  = classes.index(\"spam\")\n",
    "    i_ham   = classes.index(\"ham\")\n",
    "    \n",
    "    # 2. Retrieve the learned log probabilities for each word and class\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    log_pw = nb.feature_log_prob_\n",
    "    \n",
    "    # 3. Compute the log-odds for each feature: spam minus ham\n",
    "    log_odds = log_pw[i_spam] - log_pw[i_ham]\n",
    "    \n",
    "    # 4. Build a small DataFrame for inspection\n",
    "    df_weights = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"logP_w_given_spam\": log_pw[i_spam],\n",
    "        \"logP_w_given_ham\":  log_pw[i_ham],\n",
    "        \"logodds_spam_minus_ham\": log_odds,\n",
    "        \"odds_ratio\": np.exp(log_odds)  # x times more likely to be of that class\n",
    "    }).sort_values(\"logodds_spam_minus_ham\", ascending=False)\n",
    "    \n",
    "    # 5. Display the top indicative words for each class\n",
    "\n",
    "    top_spam = df_weights.head(topk)                 # most spam-indicative\n",
    "    top_ham  = df_weights.tail(topk).iloc[::-1]      # most ham-indicative\n",
    "\n",
    "    if (verbose):\n",
    "        print(\"Top spam-indicative features:\")\n",
    "        display(top_spam[[\"feature\", \"logodds_spam_minus_ham\", \"odds_ratio\"]])    \n",
    "        print(\"\\nTop ham-indicative features:\")\n",
    "        display(top_ham[[\"feature\", \"logodds_spam_minus_ham\", \"odds_ratio\"]])\n",
    "    else:    \n",
    "        spam_words = \", \".join(top_spam[\"feature\"].tolist())\n",
    "        ham_words  = \", \".join(top_ham[\"feature\"].tolist())\n",
    "        print(f\"Top spam-indicative words ({topk}):\\n {spam_words}\")\n",
    "        print()\n",
    "        print(f\"Top ham-indicative words  ({topk}):\\n {ham_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2690536-9f71-4270-be4c-c6b7b3c1935a",
   "metadata": {},
   "source": [
    "## Task 3. Implement NB with advanced pre-processing\n",
    "\n",
    "**Goal**: design and test your own preprocessing pipeline, and features.\n",
    "\n",
    "**Steps**:\n",
    "1. Implement any additional pre-processing steps you hypothesised before\n",
    "2. Encode additional non word features, after exploring the dataset\n",
    "3. Keep track on the effect of those changes in the performance.\n",
    "\n",
    "**Deliverable**:\n",
    "- Changes to the code reflecting your experiments\n",
    "- Log of performance of your exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4669922-49ce-4a05-a8ac-5105b7442d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, html, unicodedata\n",
    "\n",
    "def spacy_tokenizer(text, do_normalise=True):\n",
    "    \"\"\"\n",
    "    Custom tokenizer with lemmas\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space or token.is_stop:\n",
    "            continue\n",
    "\n",
    "        lemma = token.lemma_ if do_normalise else token.text\n",
    "        tokens.append(lemma)\n",
    "\n",
    "    return [t for t in tokens if t]\n",
    "    \n",
    "\n",
    "def run_nb_pipeline(custom_tokenizer, class_prior = None):\n",
    "    \"\"\"Train NB. class_prior = [P(ham), P(spam)] or None to learn from data.\"\"\"\n",
    "    vectorizer = CountVectorizer(tokenizer=custom_tokenizer, stop_words=None, token_pattern=None)\n",
    "    X_train_bow = vectorizer.fit_transform(X_train)\n",
    "    X_test_bow  = vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "    \n",
    "    nb = MultinomialNB(alpha=1.0, class_prior=class_prior)\n",
    "    nb.fit(X_train_bow, y_train)\n",
    "    \n",
    "    y_pred = nb.predict(X_test_bow)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_estimator(nb, X_test_bow, y_test, cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix – Naïve Bayes Spam Classifier with custom pre-processing \")\n",
    "    plt.show()\n",
    "\n",
    "    # We print out the top indicative words\n",
    "    print_indicative_features(nb, vectorizer, topk=300, verbose=False)\n",
    "\n",
    "    return (nb, vectorizer, y_pred)\n",
    "\n",
    "nb_a, vec_a, y_pred_a = run_nb_pipeline(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b61386-bd56-4698-97a7-503830a6009b",
   "metadata": {},
   "source": [
    "## Task 4. Explore changes in priors and tau \n",
    "**Goal**: define custom decision threshold for the classification probabilities\n",
    "\n",
    "**Steps**:\n",
    "1. Implement a revised version of `run_nb_pipeline()` that performs classification based on a given threshold (tau)\n",
    "2. Evaluate your pipeline with values (tau=60, tau=80)\n",
    "\n",
    "**Deliverable**:\n",
    "- Implemented revised function `run_nb_pipeline()`\n",
    "- Log of performance for the given values of tau\n",
    "- Reflection on the impact of those values on the classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd6218-985a-4e5f-93be-d260846b81ec",
   "metadata": {},
   "source": [
    "## Log of performance\n",
    "Paste below the performance results you obtain from your exploration. Describe the parameters or processing that led to the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e2d25-bd99-4598-bbf2-5c59d04cfaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
