{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb8df01-73c6-414d-8ee4-355768451110",
   "metadata": {},
   "source": [
    "# Praktikum 3: Applications of Word Embeddings\n",
    "\n",
    "In this Praktikum, we compare different **text representations** in a document retrieval setting — a **FAQ retrieval task**.  \n",
    "We will work on a slice of the `clips/mfaq` dataset, which contains multilingual question–answer pairs collected from various online domains.\n",
    "\n",
    "The goal is to explore how different representations influence retrieval quality. In particular, you will:\n",
    "\n",
    "- Apply **pre-trained word embeddings** to represent text semantically.  \n",
    "- Build a **FAQ retrieval system** using embeddings in place of TF-IDF.  \n",
    "- Compare **dense (semantic)** and **sparse (lexical)** representations to understand their respective strengths and limitations.\n",
    "\n",
    "Throughout the notebook, you will be asked to complete small implementation or reflection tasks that guide you through the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26399ce3-02dc-4d96-b4b7-071a56910fce",
   "metadata": {},
   "source": [
    "## 1. Setup and Base Code\n",
    "\n",
    "### 1.1 Preparing the corpus\n",
    "\n",
    "We start by downloading we are going to use in this Praktikum. the `clips/mfaq` dataset is multilingual...\n",
    "\n",
    "If you have access to internet from your notebook, simply use `download_dataset(\"clips/mfaq\")`\n",
    "Otherwise, go to the terminal, at the root of the repository and run:\n",
    "\n",
    "```bash\n",
    "$ python scripts/download_dataset.py clips/mfaq en  data/clips_mfaq_en\n",
    "```\n",
    "\n",
    "This will save the dataset in `data/clips_mafaq_en`, and you can use `load_from_disk()` referencing to the relative path to the dataset to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b617bf-6291-4470-a753-602a166a1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "dataset_folder = \"clips_mfaq_en\"\n",
    "\n",
    "ds = load_from_disk(\"../../data/\" + dataset_folder)\n",
    "df =  ds[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd6404-5fa3-418e-8de4-b25d1c7bf404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can explore the dataset here\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655b27d-6e54-4008-b7b1-13126162f40c",
   "metadata": {},
   "source": [
    "#### Preparing Question–Answer Pairs\n",
    "\n",
    "Each row in the dataset contains a list of **question–answer pairs** for one domain.  \n",
    "To evaluate different text representations, we first need to **flatten** these lists into two aligned arrays:\n",
    "\n",
    "- **QUESTIONS** – used as *queries* in our retrieval task  \n",
    "- **ANSWERS** – used to build the *document representations* (our searchable corpus)\n",
    "\n",
    "During evaluation, the goal will be to test whether each question retrieves **its corresponding answer** as the top-ranked result.\n",
    "\n",
    "\n",
    "**Important note.**\n",
    "\n",
    "We can control the **characteristics of the dataset** by adjusting the parameters of `build_qa_simple()`:\n",
    "\n",
    "- `min_len` and `max_len` — limit the **length (in characters)** of the question and answer pairs included.  \n",
    "  This helps us test how **document length** influences the performance of TF-IDF and embeddings.  \n",
    "- The number of rows from `df[\"qa_pairs\"]` passed to the function determines how many **domains** we include.  \n",
    "  Each row corresponds to one domain containing many QA pairs.\n",
    "- `max_pairs` — optionally restricts the total number of QA pairs extracted, useful for faster experimentation.\n",
    "\n",
    "These parameters let us build smaller, controlled subsets of the dataset to explore how dataset **size** and **text length** affect different retrieval methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699f8eb-c750-46a4-8ba3-3cc3baf025de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_qa_simple(qa_series, min_len=None, max_len=None, dedup=True, max_pairs=None):\n",
    "    \"\"\"\n",
    "    Flatten a Series of qa_pairs into QUESTIONS/ANSWERS.\n",
    "    We expect that  each cell is:\n",
    "      - Each cell is list-like (np.ndarray) of dicts.\n",
    "      - Each dict has 'question' and 'answer' (strings).\n",
    "    Returns QUESTIONS, ANSWERS, df_pairs.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for cell in qa_series:\n",
    "        # accept ndarray; skip anything else\n",
    "        if isinstance(cell, np.ndarray):\n",
    "            items = cell.tolist()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        for d in items:\n",
    "            if not isinstance(d, dict):\n",
    "                continue\n",
    "            q, a = d.get(\"question\"), d.get(\"answer\")\n",
    "\n",
    "            # tiny fallback if values are dicts like {'text': ...}\n",
    "            if isinstance(q, dict): q = q.get(\"text\")\n",
    "            if isinstance(a, dict): a = a.get(\"text\")\n",
    "\n",
    "            if not q or not a:\n",
    "                continue\n",
    "\n",
    "            # length filters (if given)\n",
    "            if max_len is not None and (len(q) > max_len or len(a) > max_len):\n",
    "                continue\n",
    "            if min_len is not None and (len(q) < min_len or len(a) < min_len):\n",
    "                continue\n",
    "\n",
    "            rows.append((q.strip(), a.strip()))\n",
    "\n",
    "    df_pairs = pd.DataFrame(rows, columns=[\"question\", \"answer\"])\n",
    "    if dedup and not df_pairs.empty:\n",
    "        df_pairs = df_pairs.drop_duplicates(subset=[\"question\", \"answer\"], keep=\"first\")\n",
    "    if max_pairs is not None and not df_pairs.empty:\n",
    "        df_pairs = df_pairs.head(max_pairs)\n",
    "\n",
    "    QUESTIONS = df_pairs[\"question\"].tolist()\n",
    "    ANSWERS   = df_pairs[\"answer\"].tolist()\n",
    "    return QUESTIONS, ANSWERS, df_pairs\n",
    "\n",
    "\n",
    "# Build from the top 10 domains rows (as you intended)\n",
    "QUESTIONS, ANSWERS, df_pairs = build_qa_simple(df[\"qa_pairs\"].head(150), max_len=45, min_len=30)\n",
    "print(f\"Pairs extracted: {len(QUESTIONS)}\")\n",
    "display(df_pairs.head(3))\n",
    "\n",
    "DOCUMENT_CORPUS = ANSWERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31ad9b-5f60-4eb0-a51f-73c1f9e7634c",
   "metadata": {},
   "source": [
    "### 1.2. Building Representations\n",
    "\n",
    "#### A. Tokenization and Preprocessing\n",
    "\n",
    "Before we can build any document representation, we need to **tokenize** and **normalize** the text.  \n",
    "As in previous labs, we’ll use *spaCy* to split text into tokens, remove stop words and punctuation, and lemmatize words so that related forms (e.g., *run*, *running*, *ran*) map to the same base form.\n",
    "\n",
    "The function below returns a clean list of tokens that will serve as the input for both **TF-IDF** and **embedding-based** representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae42b45-d9f1-4449-8e97-dfabd3ab72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [t.lemma_.lower() for t in doc if not t.is_stop and not t.is_punct and t.is_alpha]\n",
    "    #return [t.text.lower() for t in doc if not t.is_stop and not t.is_punct and t.is_alpha]\n",
    "\n",
    "tokenize(\"How do I change my payment method for the upcoming billing cycle?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3fc961-5ce0-4384-a80c-7d3e31337c78",
   "metadata": {},
   "source": [
    "#### B. TF-IDF-based Retrieval\n",
    "\n",
    "We first build a **TF-IDF** index over the **ANSWERS** (our document corpus).  \n",
    "Given a query (the **QUESTION**), we compute cosine similarity between the query vector and all answer vectors and return the top-k matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640ca3a8-b3ed-4c5b-87e1-2234d9b5aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# we use our tokenizer previously defined\n",
    "vectorizer_tfidf = TfidfVectorizer(tokenizer=tokenize, token_pattern=None)\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(DOCUMENT_CORPUS)\n",
    "\n",
    "def tfidf_search(query, topk=3):\n",
    "    q_vec = vectorizer_tfidf.transform([query])\n",
    "    sims = cosine_similarity(q_vec, tfidf_matrix)[0]\n",
    "    return (pd.DataFrame({\"similarity\": sims, \"faq\": DOCUMENT_CORPUS})\n",
    "              .sort_values(\"similarity\", ascending=False)\n",
    "              .head(topk))\n",
    "\n",
    "# Example\n",
    "tfidf_search(\"I can’t remember my password and need to recover access\", topk=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab5270-9899-4eae-87e7-2340d8d20570",
   "metadata": {},
   "source": [
    "#### C. Embedding-based Retrieval\n",
    "Next, we use **pre-trained word embeddings** to represent the meaning of words in our texts.  \n",
    "We’ll use the **GloVe** model trained on Wikipedia and Gigaword (`glove-wiki-gigaword-50`), which maps each word to a 50-dimensional vector.  \n",
    "\n",
    "The code below, which we also used in our seminar, loads the model from a local folder (if available) or downloads it automatically using **Gensim**.\n",
    "Refer to the `VL06_Embeddings.ipynb` notebook for information on how to download the model locally from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd19bf-18f6-41e8-ac0c-11af5b7a6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the GEMSIM_DATA_DIR to the expected local path\n",
    "GENSIM_DATA_DIR = os.path.abspath(\"../../models\")\n",
    "os.environ[\"GENSIM_DATA_DIR\"] = GENSIM_DATA_DIR\n",
    "\n",
    "#Verify if the model file is locally present\n",
    "model_name = \"glove-wiki-gigaword-50\"\n",
    "model_path = os.path.join(GENSIM_DATA_DIR, model_name)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model found at: {model_path}. We will load the local model.\")\n",
    "else:\n",
    "    print(f\"Model not found at {model_path}. Attempting to download  `{model_name}`.\")\n",
    "\n",
    "# Loading the model from its files\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa5626-0c26-43c2-adf7-6a96f6d838e2",
   "metadata": {},
   "source": [
    "**Weighted-Pooled Embeddings & Cosine Search.**  We represent each **answer** with a single vector by:\n",
    "1) tokenizing the text,  \n",
    "2) looking up each token’s pre-trained embedding, and  \n",
    "3) taking a **TF-IDF–weighted average** of those vectors (tokens with higher IDF contribute more).\n",
    "\n",
    "At query time, we embed the **question** the same way and rank answers by **cosine similarity**.\n",
    "\n",
    "Note: We’ll later implement the **mean-pooled** variant as a student task and compare both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14037e66-694c-4d59-88fe-fd835f57c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = dict(zip(vectorizer_tfidf.get_feature_names_out(), vectorizer_tfidf.idf_))\n",
    "\n",
    "def doc_vector_tfidf_weighted(text, model=model, idf=idf):\n",
    "    toks = tokenize(text)\n",
    "    vecs, weights = [], []\n",
    "    for t in toks:\n",
    "        if t in model.key_to_index and t in idf:\n",
    "            vecs.append(model[t])\n",
    "            weights.append(idf[t])\n",
    "    if not vecs:\n",
    "        return np.zeros(model.vector_size, dtype=np.float32)\n",
    "    return np.average(np.vstack(vecs), axis=0, weights=np.asarray(weights, dtype=np.float32))\n",
    "\n",
    "emb_tfidf_matrix = np.vstack([doc_vector_tfidf_weighted(a) for a in DOCUMENT_CORPUS])\n",
    "\n",
    "def hybrid_search(query_text, topk=3):\n",
    "    q_vec = doc_vector_tfidf_weighted(query_text).reshape(1, -1)\n",
    "    sims = cosine_similarity(q_vec, emb_tfidf_matrix)[0]\n",
    "    order = np.argsort(sims)[::-1][:topk]\n",
    "    return pd.DataFrame({\n",
    "        \"similarity\": sims[order],\n",
    "        \"faq\": [DOCUMENT_CORPUS[i] for i in order]\n",
    "    })\n",
    "    \n",
    "hybrid_search(\"I can’t remember my password and need to recover access\", topk=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ae765-33f1-4a90-8a34-20e3d1b23cbc",
   "metadata": {},
   "source": [
    "### 1.3 Evaluating a Retrieval Model\n",
    "\n",
    "#### A. Top-1 accuracy\n",
    "We evaluate a search function by **Top-1 accuracy**: for each *question i*, does the system rank its **corresponding answer i** as the top result?\n",
    "\n",
    "- `search_fn(query, topk)` must return a DataFrame with a `faq` column ordered by similarity (best first).\n",
    "- We compare returned text to the gold **answer text** (string match with simple normalization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85ba4ce-1fc8-4849-9015-9a3e722072ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_top_1_accuracy(\n",
    "    search_fn, queries, answers, verbose=False, normalize_text=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Benchmarks a retrieval function using Top-1 accuracy.\n",
    "\n",
    "    - search_fn(query, topk) must return a DataFrame with a 'faq' column (top-ranked first).\n",
    "    - 'answers' should be the gold answer *texts* aligned with queries.\n",
    "    \"\"\"\n",
    "    def norm(s):\n",
    "        if not normalize_text or not isinstance(s, str): \n",
    "            return s\n",
    "        return \" \".join(s.lower().split())\n",
    "\n",
    "    correct, total = 0, len(queries)\n",
    "\n",
    "    for i, q in enumerate(queries):\n",
    "        results = search_fn(q, topk=1)\n",
    "        if results is None or len(results) == 0:\n",
    "            if verbose:\n",
    "                print(f\"\\n[WARN] No results for query: {q}\")\n",
    "            continue\n",
    "\n",
    "        # Top-1 by position, independent of index labels\n",
    "        top_answer = results.iloc[0][\"faq\"]\n",
    "\n",
    "        if norm(top_answer) == norm(answers[i]):\n",
    "            correct += 1\n",
    "        elif verbose:\n",
    "            print(f\"\\nQuery: {q}\\nTop prediction: {top_answer}\\nExpected: {answers[i]}\")\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "calculate_top_1_accuracy(tfidf_search, QUESTIONS, DOCUMENT_CORPUS, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923c489-9048-4fd2-a3b9-adb95502ba38",
   "metadata": {},
   "source": [
    "#### B. Top-k Accuracy (Recall@k)\n",
    "Besides Top-1, many retrieval scenarios care whether the **correct answer appears anywhere in the top-k** results:\n",
    "- *Human-in-the-loop UIs** (FAQ search, support agents): showing the right answer in the **top 3–5** is often sufficient.\n",
    "- *Exploratory search / recommendations*: users scan several options, so **Recall@k** is the key metric.\n",
    "\n",
    "So additionally, we’ll use **Top-k accuracy (Recall@k)**: for each question *i*, check if its gold answer *i* is present in the first *k* retrieved items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb0867-704f-46c3-af87-023228a88e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_k_accuracy(search_fn, queries, answers, k=3, normalize_text=True):\n",
    "    def norm(s):\n",
    "        if not normalize_text or not isinstance(s, str):\n",
    "            return s\n",
    "        return \" \".join(html.unescape(s).lower().split())\n",
    "\n",
    "    correct = 0\n",
    "    for i, q in enumerate(queries):\n",
    "        df = search_fn(q, topk=k)\n",
    "        if df is None or len(df) == 0:\n",
    "            continue\n",
    "        gold = norm(answers[i])\n",
    "        preds = [norm(a) for a in df[\"faq\"].tolist()]\n",
    "        if gold in preds:\n",
    "            correct += 1\n",
    "    return correct / len(queries)\n",
    "\n",
    "calculate_top_k_accuracy(tfidf_search, QUESTIONS, DOCUMENT_CORPUS, k=3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168796a-e443-4c62-a02e-d66016305277",
   "metadata": {},
   "source": [
    "## 2. Tasks and exploration\n",
    "\n",
    "### Task 1. Implement Mean-Pooled Embeddings\n",
    "\n",
    "Implement `doc_vector_mean(text)` that:\n",
    "- tokenizes the text,  \n",
    "- keeps only in-vocabulary tokens,  \n",
    "- returns the **mean** of their embeddings (zeros if no valid tokens).\n",
    "\n",
    "Then:\n",
    "- build `emb_matrix` with your function,  \n",
    "- implement `embed_search(query, topk=3)` mirroring `hybrid_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9f802-cdff-41a5-ae41-ebf27fe8d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doc_vector_mean(text, model=model):\n",
    "    # TODO: tokenize, keep tokens in model.key_to_index,\n",
    "    #       return mean of embeddings (or zeros if none)\n",
    "    return None\n",
    "\n",
    "# Build matrix for ANSWERS using mean pooling\n",
    "emb_matrix = np.vstack([doc_vector_mean(a) for a in DOCUMENT_CORPUS])\n",
    "\n",
    "def embed_search(query, topk=3):\n",
    "    # TODO: embed query with doc_vector_mean,\n",
    "    #       cosine vs emb_matrix, return top-k DataFrame like hybrid_search\n",
    "    return None\n",
    "\n",
    "# Quick check to see if the output has the proper dimensions\n",
    "v = doc_vector_mean(\"short example\")\n",
    "assert v is not None and v.shape == (model.vector_size,)\n",
    "\n",
    "embed_search(\"I can’t remember my password and need to recover access\", topk=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf290e8-4615-4043-aae2-a84a84989aa6",
   "metadata": {},
   "source": [
    "### Task 2. Analysing Retrieval Performance\n",
    "\n",
    "Now that you have implemented both retrieval methods (**TF-IDF** and **Embeddings**), let’s explore how their performance changes under different conditions.\n",
    "\n",
    "#### A. Vary document length\n",
    "   - Use the parameters `min_len` and `max_len` in `build_qa_simple()` to create **small**, **medium**, and **large** answer sets.  \n",
    "   - Observe how accuracy changes with length.  \n",
    "\n",
    " **Question**: *At what point does the mean-pooled embedding start to get “diluted”?*\n",
    "\n",
    "#### B. Test lemmatization:\n",
    "   - In your `tokenize()` function, try switching between: `t.lemma_.lower()` and `t.text.lower()`\n",
    "   - Re-run your evaluation\n",
    "\n",
    "**Question**: Does lemmatization help or hurt? How does it impact the different representations, and why might that be?\n",
    "\n",
    "#### C. Compare Top-1 and Top-k accuracy\n",
    "   Use `calculate_top_1_accuracy()` and `calculate_top_k_accuracy()` to compare the performance of embeddings.\n",
    "   \n",
    "**Question**: *What model benefit more from allowing multiple results?*\n",
    "\n",
    "\n",
    "The cell below runs all experiments (TF-IDF, weighted embeddings, and mean-pooled embeddings) for **small**, **medium**, and **large** document lengths in one go. This makes it easier to compare how each method behaves as text length increases.  \n",
    "*Note:* execution may take 1-5min depending number of datasets / dataset size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3020ce-ea29-4df5-a225-ed153c2d203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "N_DOMAIN_ROWS = 100     \n",
    "TOPK = 3                          # 1 for Top-1; try 3 or 5 for Recall@k\n",
    "\n",
    "bands = { \n",
    "    \"small\":  (30,  50),   # (min_len, max_len)\n",
    "    \"medium\": (50, 200),\n",
    "    \"large\":  (200, None),\n",
    "}\n",
    "\n",
    "def run_band(min_len, max_len):\n",
    "    # 1) Rebuild QA with current length filters\n",
    "    QUESTIONS, ANSWERS, _ = build_qa_simple(\n",
    "        df[\"qa_pairs\"].head(N_DOMAIN_ROWS),\n",
    "        min_len=min_len, max_len=max_len, dedup=True\n",
    "    )\n",
    "    if not QUESTIONS:\n",
    "        return None\n",
    "    DOCS = ANSWERS\n",
    "\n",
    "    # 2) Refit TF-IDF on current DOCS + search\n",
    "    vec = TfidfVectorizer(tokenizer=tokenize, token_pattern=None)\n",
    "    tfm = vec.fit_transform(DOCS)\n",
    "\n",
    "    def tfidf_s(q, topk=3):\n",
    "        sims = cosine_similarity(vec.transform([q]), tfm)[0]\n",
    "        order = np.argsort(sims)[::-1][:topk]\n",
    "        return pd.DataFrame({\"similarity\": sims[order],\n",
    "                             \"faq\": [DOCS[i] for i in order]})\n",
    "\n",
    "    # 3) Weighted embeddings (TF-IDF-weighted)\n",
    "    idf_local = dict(zip(vec.get_feature_names_out(), vec.idf_))\n",
    "\n",
    "    def doc_vec_weighted(text):\n",
    "        toks = [t for t in tokenize(text) if t in model.key_to_index and t in idf_local]\n",
    "        if not toks:\n",
    "            return np.zeros(model.vector_size, dtype=np.float32)\n",
    "        vecs = np.vstack([model[t] for t in toks])\n",
    "        wts  = np.asarray([idf_local[t] for t in toks], dtype=np.float32)\n",
    "        return np.average(vecs, axis=0, weights=wts)\n",
    "\n",
    "    emb_w = np.vstack([doc_vec_weighted(a) for a in DOCS])\n",
    "\n",
    "    def emb_w_s(q, topk=3):\n",
    "        qv = doc_vec_weighted(q).reshape(1, -1)\n",
    "        sims = cosine_similarity(qv, emb_w)[0]\n",
    "        order = np.argsort(sims)[::-1][:topk]\n",
    "        return pd.DataFrame({\"similarity\": sims[order],\n",
    "                             \"faq\": [DOCS[i] for i in order]})\n",
    "\n",
    "    # 4) Mean-pooled embeddings \n",
    "    def doc_vec_mean(text):\n",
    "        toks = [t for t in tokenize(text) if t in model.key_to_index]\n",
    "        if not toks:\n",
    "            return np.zeros(model.vector_size, dtype=np.float32)\n",
    "        return np.mean(np.vstack([model[t] for t in toks]), axis=0)\n",
    "\n",
    "    emb_mean = np.vstack([doc_vec_mean(a) for a in DOCS])\n",
    "\n",
    "    def emb_mean_s(q, topk=3):\n",
    "        qv = doc_vec_mean(q).reshape(1, -1)\n",
    "        sims = cosine_similarity(qv, emb_mean)[0]\n",
    "        order = np.argsort(sims)[::-1][:topk]\n",
    "        return pd.DataFrame({\"similarity\": sims[order],\n",
    "                             \"faq\": [DOCS[i] for i in order]})\n",
    "\n",
    "    # 5) Metrics\n",
    "    row = {\n",
    "        \"n_pairs\": len(DOCS),\n",
    "        \"TF-IDF@1\":        calculate_top_1_accuracy(tfidf_s,   QUESTIONS, DOCS),\n",
    "        \"Emb(TFIDFw)@1\":   calculate_top_1_accuracy(emb_w_s,   QUESTIONS, DOCS),\n",
    "        \"Emb(mean)@1\":     calculate_top_1_accuracy(emb_mean_s,QUESTIONS, DOCS),\n",
    "    }\n",
    "    if TOPK and TOPK > 1:\n",
    "        row.update({\n",
    "            f\"TF-IDF@{TOPK}\":      calculate_top_k_accuracy(tfidf_s,    QUESTIONS, DOCS, k=TOPK),\n",
    "            f\"Emb(TFIDFw)@{TOPK}\": calculate_top_k_accuracy(emb_w_s,    QUESTIONS, DOCS, k=TOPK),\n",
    "            f\"Emb(mean)@{TOPK}\":   calculate_top_k_accuracy(emb_mean_s, QUESTIONS, DOCS, k=TOPK),\n",
    "        })\n",
    "    return row\n",
    "\n",
    "rows = []\n",
    "for name, (mn, mx) in bands.items():\n",
    "    r = run_band(mn, mx)\n",
    "    if r is not None:\n",
    "        r[\"band\"] = name\n",
    "        r[\"range\"] = f\"[{mn or 0}, {mx or '∞'}]\"\n",
    "        rows.append(r)\n",
    "\n",
    "df_len = pd.DataFrame(rows).set_index(\"band\")\n",
    "display(df_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c920f0a-d318-4ceb-bc5d-0715aa25232f",
   "metadata": {},
   "source": [
    "### Reporting your results\n",
    "\n",
    "Summarize your observations clearly and concisely in **three short paragraphs or tables**, one per experiment:\n",
    "\n",
    "A. **Effect of document length:**  \n",
    "   - Report the Top-1 accuracy for *small*, *medium*, and *large* subsets.  \n",
    "   - Highlight where the **embedding performance starts to drop** (semantic dilution).  \n",
    "   - You may present this in a small table like:\n",
    "\n",
    "     | Representation | Small | Medium  | Large  |\n",
    "     |----------------|-------|---------|--------|\n",
    "     | TF-IDF         | 0.xxx | 0.xxx   | 0.xxx  |\n",
    "     | Embeddings     | 0.xxx | 0.xxx   | 0.xxx  |     \n",
    "\n",
    "B. **Effect of lemmatization:**  \n",
    "   - Run both versions of `tokenize()` (lemma vs. raw text).\n",
    "   - Fix the comparison under one dataset size (e.g., medium)  \n",
    "   - Compare TF-IDF and embeddings.  \n",
    "   - Write 2–3 sentences explaining **why** lemmatization might help or hurt the models.\n",
    "  \n",
    "     | Representation | Raw   | Lemma   | \n",
    "     |----------------|-------|---------|\n",
    "     | TF-IDF         | 0.xxx | 0.xxx   |\n",
    "     | Embeddings     | 0.xxx | 0.xxx   |\n",
    "\n",
    "C. **Effect of Top-k retrieval:**  \n",
    "   - Report Top-1 and Top-k (e.g., k=3 or 5) accuracy side-by-side.  \n",
    "   - Briefly answer: *Which representation benefits more from allowing multiple retrieved answers?*  \n",
    "   - Example table:\n",
    "\n",
    "     | Representation | Top-1 | Recall@3 | Recall@5 |\n",
    "     |----------------|-------|-----------|----------|\n",
    "     | TF-IDF         | 0.xxx | 0.xxx     | 0.xxx    |\n",
    "     | Embeddings     | 0.xxx | 0.xxx     | 0.xxx    |\n",
    "\n",
    "\n",
    "Keep your answers short and interpretive — focus on describing your interpretations, not just repeating raw numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2badbc08-f2a6-4be1-a7c4-a70b003be543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
