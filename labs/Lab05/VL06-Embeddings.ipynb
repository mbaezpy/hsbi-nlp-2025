{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4babac06-f4cf-4f96-96c7-d87d3d07b7ee",
   "metadata": {},
   "source": [
    "# VL06 – Embeddings and Vector Semantics\n",
    "\n",
    "In this seminar, we move beyond sparse count-based models (like Bag-of-Words and TF-IDF)  \n",
    "to explore **dense vector representations** of language.  \n",
    "\n",
    "We will experiment hands-on with pre-trained embeddings to:\n",
    "- measure **similarity** and **analogy** between words,  \n",
    "- visualize semantic neighborhoods, and  \n",
    "- build simple **document representations** using embedding averages and TF-IDF weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b54438-1d35-4fed-92a8-2affb5043158",
   "metadata": {},
   "source": [
    "## 1. Loading Pre-trained Word Embeddings\n",
    "\n",
    "Pre-trained embeddings are word vectors that have already been learned from a very large corpus (e.g., Wikipedia, Gigaword).  \n",
    "We’ll use them to represent words as **dense vectors** without training anything ourselves.\n",
    "\n",
    "**Step 1 – Install and prepare Gensim**\n",
    "\n",
    "Activate your Conda environment and install **Gensim**, a popular Python library for NLP vector models:\n",
    "\n",
    "```bash\n",
    "$ pip install gensim\n",
    "```\n",
    "\n",
    "**Step 2 – Download the model (only once)**\n",
    "If your notebook **has internet access**, you can download directly with Gensim’s built-in downloader.  \n",
    "Otherwise, download from the terminal using the provided script:\n",
    "\n",
    "```bash\n",
    "$ python scripts/download_gensim.py glove-wiki-gigaword-50 models/\n",
    "```\n",
    "\n",
    "This script saves the model under `models/glove-wiki-gigaword-50/`.\n",
    "\n",
    "**Step 3 – Set the environment variable**\n",
    "We tell Gensim where to find the downloaded models by setting the environment variable `GENSIM_DATA_DIR`. We do this in the code below right before importing gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6e671-548d-4f68-b05a-b67e7e57cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the GEMSIM_DATA_DIR to the expected local path\n",
    "GENSIM_DATA_DIR = os.path.abspath(\"../../models\")\n",
    "os.environ[\"GENSIM_DATA_DIR\"] = GENSIM_DATA_DIR\n",
    "\n",
    "#Verify if the model file is locally present\n",
    "model_name = \"glove-wiki-gigaword-50\"\n",
    "model_path = os.path.join(GENSIM_DATA_DIR, model_name)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model found at: {model_path}. We will load the local model.\")\n",
    "else:\n",
    "    print(f\"Model not found at {model_path}. Attempting to download  `{model_name}`.\")\n",
    "\n",
    "\n",
    "# Loading the model from its files\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(model_name)\n",
    "model.most_similar(\"apricot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa155202-6f23-4029-b6bd-5493d3fbbd8a",
   "metadata": {},
   "source": [
    "## 2. Inspecting the Model and Vectors\n",
    "\n",
    "We’ll quickly **summarize the embedding model** (vocab size, dimensionality, memory), then **probe the vocabulary** and **peek at a word vector**.  \n",
    "Where available, we’ll also check the **token count** stored by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8098f9c-cade-4c30-979c-5f691685764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary helper: prints key facts about a Gensim KeyedVectors model\n",
    "import numpy as np\n",
    "\n",
    "def summarize_kv(model, name=\"(unknown)\"):    \n",
    "    print(f\"=== Model info: {name} ===\")\n",
    "    print(\"Type:            \", type(model).__name__)\n",
    "    print(\"Vector size (d): \", model.vector_size)\n",
    "    print(\"Vocab size (|V|):\", len(model))\n",
    "    \n",
    "    # matrix shapes (if present)\n",
    "    if hasattr(model, \"vectors\"):\n",
    "        print(\"Matrix shape:    \", model.vectors.shape, \"(rows = |V|, cols = d)\")\n",
    "        print(\"Memory (approx): \", f\"{model.vectors.nbytes/1e6:.1f} MB (vectors only)\")\n",
    "    # first few tokens (sorted by frequency order if available)\n",
    "    print(\"Top 10 tokens:   \", model.index_to_key[:10])\n",
    "\n",
    "    # norms cache info: is the model pre-computing ||v||\n",
    "    try:\n",
    "        nv = model.get_normed_vectors()  # computes/caches if missing\n",
    "        print(\"Norms cached:    \", \"yes\" if nv is not None else \"no\")\n",
    "    except Exception:\n",
    "        print(\"Norms cached:     (n/a)\")\n",
    "\n",
    "summarize_kv(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f1af7-2e7b-428a-a40d-ccdf200e44c8",
   "metadata": {},
   "source": [
    "### 2.1 Is a word in the vocabulary?\n",
    "\n",
    "Check presence and, if present, its integer index in the model’s vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f06d277-933c-4f34-9df0-6da41a8e0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"apricot\"  #  <- try changing this\n",
    "in_vocab = word in model.key_to_index\n",
    "\n",
    "print(f\"'{word}' in vocabulary?  {in_vocab}\")\n",
    "if in_vocab:\n",
    "    print(\"Index:\", model.key_to_index[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4535bdf-f3bb-4eb0-8ab7-b0a75284862f",
   "metadata": {},
   "source": [
    "### 2.2 Access the vector\n",
    "Vectors are NumPy arrays of length **d**. Let's access the previous 'word' vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f2921-7ac9-414b-a576-8da33a3d5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's access it's vector representation\n",
    "v = model[word]\n",
    "print (v)\n",
    "print(\"Vector shape:\", v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c5a35-c942-404c-977d-3d17b0085224",
   "metadata": {},
   "source": [
    "### 2.3 (If available) How frequent is this token?\n",
    "\n",
    "Some pretrained packages include token **counts** from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba13e1-9741-4b17-af71-926ad45be517",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    count = model.get_vecattr(word, \"count\")\n",
    "    print(f\"Word '{word}' appears {count} times in the training corpus\")\n",
    "except Exception:    \n",
    "    print (\"No counts available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c31f6f-1f02-4d32-87ea-269c416b8b6f",
   "metadata": {},
   "source": [
    "## 2. Similarity\n",
    "\n",
    "We measure how similar two word vectors are by the **cosine of the angle** between them:  \n",
    "$$\n",
    "\\cos(\\theta)=\\frac{\\mathbf{v}_a \\cdot \\mathbf{v}_b}{\\|\\mathbf{v}_a\\|\\,\\|\\mathbf{v}_b\\|}\n",
    "$$\n",
    "Cosine ≈ 1 → very similar (vectors point in the same direction); Cosine ≈ 0 → unrelated.\n",
    "\n",
    "### 2.1 Computing cosine similarity between vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccd157-8d53-45d0-9d21-afbb77271d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual cosine to demystify what the library computes\n",
    "def cosine_manual(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    dot = float(np.dot(a, b))\n",
    "    na  = float(np.linalg.norm(a))\n",
    "    nb  = float(np.linalg.norm(b))\n",
    "    return dot / (na * nb)\n",
    "\n",
    "w1, w2 = \"apricot\", \"peach\"    # <- try changing to (\"apricot\",\"car\")\n",
    "v1, v2 = model[w1], model[w2]\n",
    "\n",
    "cos_man = cosine_manual(v1, v2)\n",
    "print(f\"Manual cosine({w1}, {w2}) = {cos_man:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d70ec-c787-44b1-bf96-657948eed22b",
   "metadata": {},
   "source": [
    "**Check against Gensim’s API.**  \n",
    "`model.similarity(a, b)` computes the same cosine using the model’s (possibly cached) normalized vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a7e480-260b-449b-8f8b-30794df3471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare it to the built-in function\n",
    "cos_api = model.similarity(w1, w2)\n",
    "print(f\"Gensim similarity({w1}, {w2}) = {cos_api:.4f}\")\n",
    "\n",
    "# They should match up to tiny floating-point differences:\n",
    "np.testing.assert_allclose(cos_man, cos_api, rtol=1e-6, atol=1e-7) # if not, it will throw an assertion error\n",
    "print(\"Manual and built-in cosine match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e98515-5ea0-4dee-bfb6-04113d904dcc",
   "metadata": {},
   "source": [
    "### 2.2 Nearest Neighbors\n",
    "\n",
    "Given a word, `most_similar()` returns the **top-N nearest neighbors** by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fccadd-9a4d-44a4-ada0-0c4324fb7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neareast neighboards\n",
    "model.most_similar(\"king\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ed120-35a1-41fc-ab51-fc3495925180",
   "metadata": {},
   "source": [
    "## 3. Analogies and Simple Geometry\n",
    "\n",
    "“A : B :: C : ?” ≈ find the word nearest to **v(B) − v(A) + v(C)** (using cosine similarity).  \n",
    "Many relations appear as **consistent directions** in the embedding space (e.g., gender, capital–country, plant–produce).\n",
    "\n",
    "For example, given the analogy *man : king :: woman : ?*, we can express it as vector operations:\n",
    "\n",
    "$$\n",
    "v(\\text{king}) - v(\\text{man}) = v(x) - v(\\text{woman})\n",
    "$$\n",
    "\n",
    "$$\n",
    "v(x) = v(\\text{king}) - v(\\text{man}) + v(\\text{woman})\n",
    "$$\n",
    "\n",
    "In a Gensim model, this can be computed as:\n",
    "\n",
    "```python\n",
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])\n",
    "```\n",
    "\n",
    "This retrieves the word(s) whose vector **v(x)** is closest (by cosine similarity) to the computed target vector. We expect to see **queen** as a result — illustrating how linear relations can capture semantic structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed343f-b75d-478f-8b85-1030b379950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d401f8-1540-4b21-82e6-f9d0cc17ba1b",
   "metadata": {},
   "source": [
    "### 3.1 Inspect the following relationships\n",
    "\n",
    "| Vector Operation | Geometric Expression | Relation Type | Expected Result |\n",
    "|------------------|----------------------|----------------|-----------------|\n",
    "| `[\"grape\", \"tree\"] – [\"apple\"]` | **v(grape) + v(tree) − v(apple)** | “X grows on Y” → *grape : vine* as *apple : tree* | **vine** |\n",
    "| `[\"king\", \"woman\"] – [\"man\"]` | **v(king) − v(man) + v(woman)** | **Gender** swap | **queen** |\n",
    "| `[\"paris\", \"italy\"] – [\"france\"]` | **v(paris) − v(france) + v(italy)** | **Capital–country** | **rome** |\n",
    "| `[\"doctor\", \"woman\"] – [\"man\"]` | **v(doctor) − v(man) + v(woman)** | **Gender** direction applied to occupation | *(may show stereotype)* |\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation tips:**\n",
    "- Results reflect **usage patterns** (distributional similarity).  \n",
    "- Unexpected outputs can indicate **polysemy**, **frequency effects**, or **bias** in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193cdda4-a9c1-46f0-beec-65dec22eb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    # --- PLANT / PRODUCE RELATION (grow-on) ---\n",
    "    ([\"grape\", \"tree\"], [\"apple\"]),        # grape : vine :: apple : tree  → expect \"vine\"\n",
    "\n",
    "    # --- GENDER DIRECTION / ROLE SWAP ---\n",
    "    ([\"king\", \"woman\"], [\"man\"]),          # gender swap -> expect \"queen\"\n",
    "    ([\"doctor\", \"woman\"], [\"man\"]),        # occupation with gender direction -> may reveal bias\n",
    "\n",
    "    # --- GEO-POLITICAL RELATIONS ---\n",
    "    ([\"paris\", \"italy\"], [\"france\"]),      # capital–country -> expect \"rome\"\n",
    "    ([\"german\", \"spain\"], [\"germany\"]),    # demonym/language transfer -> expect \"spanish\" (or \"spaniard\")\n",
    "\n",
    "    # --- MORPHOLOGY / GRAMMAR ---\n",
    "    ([\"faster\", \"slow\"], [\"fast\"]),        # comparative transfer -> expect \"slower\"\n",
    "    ([\"walked\", \"run\"], [\"walk\"]),         # past tense transfer -> expect \"ran\"\n",
    "    ([\"cups\", \"chair\"], [\"cup\"]),          # pluralization transfer -> expect \"chairs\"\n",
    "\n",
    "    # --- SEMANTIC HIERARCHY / HYPERNYM TRANSFER (noisier) ---\n",
    "    ([\"animal\", \"tree\"], [\"dog\"]),         # dog -> animal applied to tree -> expect \"plant\" \n",
    "    ([\"apple\", \"iphone\"], [\"fruit\"]),      # tricky one with static vectors (apple has two meanings but one vector)\n",
    "\n",
    "    # --- PART–WHOLE / MERONYMY TRANSFER (often noisy) ---\n",
    "    ([\"wheel\", \"arm\"], [\"car\"]),           # car→wheel applied to arm -> expect \"hand\" \n",
    "]\n",
    "\n",
    "for pos, neg in tests:\n",
    "    ans = model.most_similar(positive=pos, negative=neg, topn=5)\n",
    "    print(f\"\\n{pos} - {neg}  ->  {[f'{w}:{s:.2f}' for w,s in ans]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab1136-c939-4e9a-844e-d632c6e7222b",
   "metadata": {},
   "source": [
    "### 3.2 Reflect on analogies\n",
    "1. Which types of relations (e.g., gender, grammar, geography) seem to work best in these results, and which ones break down or produce noisy answers?  \n",
    "2. Some results (like *doctor–man+woman → nurse*) reflect social or cultural patterns rather than logic.  \n",
    "   What does this tell us about the data used to train word embeddings?  \n",
    "3. When the model makes errors (e.g., *wheel–arm–car → sling*), what does that reveal about what word embeddings capture — and what they *don’t*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557bdbe-7aea-4caa-80de-7379bc830224",
   "metadata": {},
   "source": [
    "## 4. From words to documents\n",
    "\n",
    "**Goal.** Turn each FAQ into a single **document vector** so we can do semantic retrieval.  \n",
    "We’ll try two approaches:\n",
    "\n",
    "1) **Mean pooling** of word embeddings  \n",
    "2) **TF-IDF–weighted** mean pooling (to emphasize informative terms)\n",
    "\n",
    "We’ll use spaCy for tokenization/lemmatization and Gensim for word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dce952-fee3-462b-9459-98385c832207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "faqs = [\n",
    "    \"How do I reset my password, and are passwords case-sensitive?\", \n",
    "    \"How to update my email address, and is updating necessary for security?\", \n",
    "    \"I can't log in to my account. What are the common login issues?\", \n",
    "    \"How do I change my payment method for the upcoming billing cycle?\", \n",
    "    \"I want to cancel my active monthly subscription and stop future payments.\",\n",
    "    \"What is the return policy if a product is still under warranty?\",\n",
    "    \"How to change my username if I am currently logging in with my email?\",\n",
    "    \"How do I enable two-factor authentication for my account?\",\n",
    "    \"How to update the billing address associated with my credit card.\",\n",
    "    \"How do I account for changes in my order? Can I cancel it?\",\n",
    "    \"I forgot my credentials—how can I recover access to my account?\",\n",
    "    \"How do I change the card on file for future charges?\",\n",
    "    \"How can I stop auto-renewal and end my membership?\",\n",
    "    \"Do you offer refunds or exchanges, and how are returns handled?\",\n",
    "    \"Can I switch the email associated with my profile?\",\n",
    "    \"Where can I activate two-step verification for extra security?\",\n",
    "    \"How do I modify the invoice address linked to my account?\",\n",
    "    \"My sign-in keeps failing—how do I troubleshoot login problems?\",\n",
    "    \"Can I change the name that appears on my account?\",\n",
    "    \"I need to update my payment details before the next bill—how do I do that?\"\n",
    "]\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [t.lemma_.lower() for t in doc\n",
    "            if not t.is_stop and not t.is_punct and t.is_alpha]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94220dde-a27e-4c8f-a806-18fe9b3f05f8",
   "metadata": {},
   "source": [
    "### 4.1 Mean-Pooled Embedding per Document\n",
    "\n",
    "**Idea.** Average all in-vocab word vectors in a document.  \n",
    "**Pros:** simple, fast. **Cons:** treats all words equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f743e74-0677-4c0f-8011-8f0cbbd5d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def doc_vector(text, model=model):\n",
    "    toks = tokenize(text)\n",
    "    vecs = [model[t] for t in toks if t in model.key_to_index]\n",
    "    if not vecs:\n",
    "        return np.zeros(model.vector_size, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# Build matrix once\n",
    "emb_matrix = np.vstack([doc_vector(d) for d in faqs])\n",
    "\n",
    "def embed_search(query_text, topk=3):\n",
    "    q_vec = doc_vector(query_text)\n",
    "    sims = cosine_similarity(q_vec.reshape(1, -1), emb_matrix)[0]\n",
    "    return pd.DataFrame({\"similarity\": sims, \"faq\": faqs}).sort_values(\"similarity\", ascending=False).head(topk)\n",
    "\n",
    "\n",
    "embed_search(\"How do I change my password?\", topk=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4637be-60c9-43df-835b-ea34aedc4e2c",
   "metadata": {},
   "source": [
    "### 4.2 TF-IDF–Weighted Embedding per Document\n",
    "\n",
    "**Idea.** Weight each word vector by its **TF-IDF** before averaging:  \n",
    "$$\n",
    "v_{\\text{doc}} = \\frac{\\sum_{w \\in d} \\text{tfidf}(w)\\, v(w)}{\\sum_{w \\in d} \\text{tfidf}(w)}\n",
    "$$\n",
    "**Benefit.** Emphasizes informative terms (e.g., *refund*, *authentication*) while keeping semantic generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c58dc81-70b8-43d5-a223-c8093ec8fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit TF-IDF on the same corpus (important: fit before reading idf values)\n",
    "vectorizer_tfidf = TfidfVectorizer(tokenizer=tokenize, token_pattern=None)\n",
    "tfidf_matrix_dummy = vectorizer_tfidf.fit_transform(faqs)  # fit to populate vocabulary + IDF\n",
    "idf = dict(zip(vectorizer_tfidf.get_feature_names_out(), vectorizer_tfidf.idf_))\n",
    "\n",
    "\n",
    "def doc_vector_tfidf_weighted(text, model=model, idf=idf):\n",
    "    toks = tokenize(text)\n",
    "    vecs, weights = [], []\n",
    "    for t in toks:\n",
    "        if t in model.key_to_index and t in idf:\n",
    "            vecs.append(model[t])\n",
    "            weights.append(idf[t])\n",
    "    if not vecs:\n",
    "        return np.zeros(model.vector_size)\n",
    "    weights = np.array(weights)\n",
    "    return np.average(vecs, axis=0, weights=weights)\n",
    "\n",
    "# Precompute hybrid document vectors\n",
    "emb_tfidf_matrix = np.vstack([doc_vector_tfidf_weighted(doc) for doc in faqs])\n",
    "\n",
    "# Query function\n",
    "def hybrid_search(query_text, topk=3):\n",
    "    q_vec = doc_vector_tfidf_weighted(query_text)\n",
    "    sims = cosine_similarity(q_vec.reshape(1, -1), emb_tfidf_matrix)[0]\n",
    "    return pd.DataFrame({\"similarity\": sims, \"faq\": faqs}).sort_values(\"similarity\", ascending=False).head(topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772af6d-992d-47df-b6d9-20eca95ad75d",
   "metadata": {},
   "source": [
    "### 4.3 Compare Mean vs. TF-IDF–Weighted\n",
    "\n",
    "Try a few queries that have **low keyword overlap** but **high semantic overlap**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba0d19-97d8-45ef-9f09-f2f4852611a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_doc_retrieval(query_text, topk=3):\n",
    "    print(f\"\\nQUERY: {query_text}\\n\" + \"-\"*60)\n",
    "    print(\"[Embeddings: mean pooling]\")\n",
    "    display(embed_search(query_text, topk))\n",
    "    print(\"\\n[Embeddings: TF-IDF weighted]\")\n",
    "    display(hybrid_search(query_text, topk))\n",
    "\n",
    "queries = [\n",
    "    \"I can’t remember my password and need to get back into my account.\",\n",
    "    \"Please end my membership so I’m not charged again next month.\",\n",
    "    \"How can I turn on additional security when signing in?\",\n",
    "    \"Set up two-step verification for logins\"\n",
    "]\n",
    "\n",
    "# Example\n",
    "for q in queries:    \n",
    "    compare_doc_retrieval(q, topk=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719beefb-374e-40fb-8aaa-42951a9922fe",
   "metadata": {},
   "source": [
    "## 5. (Extra) Visualizing Embeddings\n",
    "Each word in the model we worked with in this seminar, is a point in a **50-dimensional** space. We can’t plot 50D, so we use **PCA (Principal Component Analysis)** to find the **two directions** that explain the most variation in these points, and then **project** the points onto those two directions. This gives a 2D picture that preserves as much of the global structure as possible with a *linear* projection.\n",
    "\n",
    "**How PCA works (intuition).**\n",
    "- Center the data - vectors of words provided (subtract the mean).\n",
    "- Find orthogonal axes that capture the **largest variance** in the data (PC1, then PC2).\n",
    "- Project each 50D vector onto these two axes to get 2D coordinates.\n",
    "\n",
    "In the resulting graph, points that are **close** in 2D are often semantically related (e.g., animals vs. vehicles). But note that with only a small set of words, the view can change a lot depending on which words you include.\n",
    "\n",
    "### 5.1 Semantic Neighborhoods \n",
    "\n",
    "We project 50-D word vectors to 2-D with **PCA** to glimpse their geometry. Notice how **animals**, **vehicles**, and **people/royalty** form separate clusters—nearer points ≈ higher semantic similarity (with some distortion from the 2-D projection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22f90e-01e5-45a9-af9d-d234fb905a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic similarity between different words\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [\"dog\",\"cat\",\"wolf\",\"tiger\",\"car\",\"truck\",\"bus\",\"train\",\"king\",\"queen\",\"man\",\"woman\"]\n",
    "X = np.vstack([model[w] for w in words])\n",
    "Z = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(Z[:,0], Z[:,1])\n",
    "for (x,y), w in zip(Z, words):\n",
    "    plt.text(x+0.01, y+0.01, w, fontsize=9)\n",
    "plt.title(\"PCA projection of selected words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97ceac-c95d-4d8c-9bd0-ffe374520497",
   "metadata": {},
   "source": [
    "### 5.2 Visualizing a Semantic Direction\n",
    "We can project a few words onto 2D (using PCA) to see if certain relationships form clear geometric patterns.  \n",
    "Here, we visualize the **gender direction** in the embedding space — words like *man–woman* or *king–queen* often align along a similar axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005e2d0-06c5-426f-b912-aced291b5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the \"gender direction\" in embedding space\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Choose words along the gender / role axis\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"doctor\", \"nurse\", \"actor\", \"actress\", \"father\", \"mother\"]\n",
    "\n",
    "# Stack their vectors\n",
    "X = np.vstack([model[w] for w in words])\n",
    "\n",
    "# Project from 50D → 2D with PCA\n",
    "Z = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(Z[:,0], Z[:,1], color=\"steelblue\")\n",
    "\n",
    "# Annotate\n",
    "for (x,y), w in zip(Z, words):\n",
    "    plt.text(x+0.01, y+0.01, w, fontsize=9)\n",
    "\n",
    "plt.title(\"Gender Direction in Embedding Space (PCA Projection)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.axhline(0, color=\"gray\", lw=0.5)\n",
    "plt.axvline(0, color=\"gray\", lw=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18aafa-5cf8-4e41-8db7-131a74034646",
   "metadata": {},
   "source": [
    "## References\n",
    "- Gensim API reference: https://radimrehurek.com/gensim/apiref.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
