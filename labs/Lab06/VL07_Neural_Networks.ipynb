{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ab08f03-3c6d-4958-9d0b-ee02c58e36ca",
   "metadata": {},
   "source": [
    "# VL07 – Logistic Regression & Neural Networks\n",
    "\n",
    "In this seminar, we explore how classical machine learning (logistic regression) and modern neural networks approach the same NLP task: **sentiment analysis**.  We will be doing the following:\n",
    "\n",
    "- training a logistic regression model\n",
    "- preparing text for neural models (tokenization, vocabulary, padding)  \n",
    "- defining simple neural architectures in PyTorch  \n",
    "- understanding forward and backward passes  \n",
    "- training with mini-batch gradient descent  \n",
    "- evaluating and comparing different models  \n",
    "\n",
    "The goal is not to build the most powerful model, but to understand **how** neural networks process text and how they differ from traditional linear classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a4e26-0ff0-4881-8468-2e1532d40170",
   "metadata": {},
   "source": [
    "## 1. Loading a Sentiment Dataset (IMDB)\n",
    "\n",
    "For the sentiment analysis task, we will be using a real-world sentiment analysis dataset:  \n",
    "**IMDB movie reviews**. This dataset is hosted in Huggingface and is offered at `stanfordnlp/imdb`. This dataset contatins a `text` column contating the document or review, and a `label` with values:\n",
    "\n",
    "- `0` = negative review  \n",
    "- `1` = positive review\n",
    "\n",
    "Use the `download_dataset.py` script if you don't have access to the Internet from your jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abafa0ce-25c1-48bf-b1e9-806711ba0b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "#ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "ds = load_from_disk(\"../../data/standfordnlp_imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b16490-4277-4ca3-a2d8-f3b9108c4d59",
   "metadata": {},
   "source": [
    "### Create train / split sets\n",
    "We take the train and test splits already provided by the dataset, and convert it to DataFrames. We can explore some properties of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608b85f-eb9f-4c39-8306-450ce43d770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert splits to pandas DataFrames\n",
    "df_train_full = ds[\"train\"].to_pandas()\n",
    "df_test_full = ds[\"test\"].to_pandas()\n",
    "\n",
    "print(\"Train shape:\", df_train_full.shape)\n",
    "print(\"Test  shape:\", df_test_full.shape)\n",
    "\n",
    "df_train_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c34837-64ed-471c-a0ec-adab8fd45ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d2821a-0d2f-4485-b878-1ac373370e05",
   "metadata": {},
   "source": [
    "### Create small datasets\n",
    "To make the training during class manageable, we can take a subset of the original train and test datasets. We can use `train_test_split()` to get a stratified sample, controlling the sample size with the `train_size` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900fcb6-4c5f-4618-b2ef-1ee052cc207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, _ = train_test_split(\n",
    "    df_train_full,\n",
    "    train_size=20_000,\n",
    "    stratify=df_train_full[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_test, _ = train_test_split(\n",
    "    df_test_full,\n",
    "    train_size=4_000,\n",
    "    stratify=df_test_full[\"label\"],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce073a1-345f-433e-b256-5e286f2e9ab5",
   "metadata": {},
   "source": [
    "## 2. Baseline Sentiment Classifier (Logistic Regression)\n",
    "\n",
    "We now build a simple baseline sentiment classifier:\n",
    "\n",
    "- Input: IMDB movie review text  \n",
    "- Representation: TF–IDF features  \n",
    "- Model: Logistic Regression (linear classifier)\n",
    "\n",
    "We will:\n",
    "1. Fit the vectorizer and model on the training split.\n",
    "2. Use the model to classify custom phrases that we provide.\n",
    "\n",
    "### 2.1 Prepare the TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6a770-c915-45d5-8ef1-d362ebccfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF–IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20_000,   # cap vocabulary size for speed\n",
    "    ngram_range=(1, 2),    # unigrams + bigrams\n",
    "    stop_words=\"english\"   # simple English stopword removal\n",
    ")\n",
    "\n",
    "# Fit vectorizer on training texts\n",
    "X_train = tfidf.fit_transform(df_train[\"text\"])\n",
    "y_train = df_train[\"label\"].values\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e959f-87f7-4fc8-9687-9ad1073166b6",
   "metadata": {},
   "source": [
    "### 2.2 Fit the Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a25a2b-77fc-4c42-8748-f1c6c101acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3881701-1247-4c04-a7f7-011cb575421d",
   "metadata": {},
   "source": [
    "### 2.3 Testing sentiment prediction\n",
    "Now that we have trained the model, we can test the sentiment prediction. The interface is similar to that of NB, being able to invoke `predict()` and `predict_proba()` to obtain the label prediction and its probaility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e79fd-0387-4bb0-8d20-5ec556de2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am very happy\"\n",
    "v = tfidf.transform([text])\n",
    "\n",
    "print (\"Prob: \", log_reg.predict_proba(v))\n",
    "print (\"Pred: \", log_reg.predict(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42581ae5-c476-481b-aeae-30fd6e3344e7",
   "metadata": {},
   "source": [
    "Below we have a helper function to predict the sentiment of an array of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7fc73-031f-4e18-b8c0-73735acf1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    \"\"\"\n",
    "    texts: list of strings (sentences or reviews)\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    X = tfidf.transform(texts)\n",
    "    probs = log_reg.predict_proba(X)\n",
    "    preds = log_reg.predict(X)\n",
    "    \n",
    "    for text, pred, prob in zip(texts, preds, probs):\n",
    "        label = label_names.get(int(pred), str(pred))\n",
    "        confidence = float(np.max(prob))\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\" → Predicted: {label} (confidence: {confidence:.3f})\")\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ba183-4f02-4ec7-ac5c-e5615c7e0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the list to test it. What if it is not movie related?\n",
    "examples = [\n",
    "    \"I absolutely loved this movie, it was fantastic!\",\n",
    "    \"This was the worst film I have seen in years.\",\n",
    "    \"The story was a bit slow, but the acting was great.\",\n",
    "    \"Not bad, but I wouldn't watch it again.\"\n",
    "]\n",
    "\n",
    "predict_sentiment(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67735220-5e20-469d-991d-7717ee5b9122",
   "metadata": {},
   "source": [
    "### 2.4 Inspecting the model\n",
    "We can have access to the learned model coeficients (weights), and visualize, wich are the terms that *explain* the positive and negative sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64bf062-52aa-4583-8cd6-c18fa8d61005",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(tfidf.get_feature_names_out())\n",
    "coefs = log_reg.coef_[0]  # since this is a binary classifier\n",
    "\n",
    "# Sort coefficients\n",
    "top_pos_indices = np.argsort(coefs)[-20:]\n",
    "top_neg_indices = np.argsort(coefs)[:20]\n",
    "\n",
    "print(\"Top POSITIVE words:\")\n",
    "for idx in top_pos_indices:\n",
    "    print(f\"{feature_names[idx]:<20}  weight= {coefs[idx]:.3f}\")\n",
    "\n",
    "print(\"\\nTop NEGATIVE words:\")\n",
    "for idx in top_neg_indices:\n",
    "    print(f\"{feature_names[idx]:<20}  weight= {coefs[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2871238-6c47-45b3-8ebf-ad66e18b0e07",
   "metadata": {},
   "source": [
    "We can also have a helper function to show the influence of each word on a specific document. We can do this by multiplying  $w_i . x_i$ or `weight * tfidf` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac4997-9c95-423c-a26b-9ad2a09a185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word_influences(text):\n",
    "    # Transform the text into tf-idf vector\n",
    "    X = tfidf.transform([text])\n",
    "    \n",
    "    # Extract non-zero features\n",
    "    indices = X.nonzero()[1]\n",
    "    words = feature_names[indices]\n",
    "    weights = coefs[indices] * X.data  # weight * tfidf value\n",
    "    \n",
    "    # Sort by absolute contribution\n",
    "    sorted_idx = np.argsort(np.abs(weights))[::-1]\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"\\nMost influential words:\")\n",
    "    print(\"-\" * 40)\n",
    "    for idx in sorted_idx[:10]:\n",
    "        w = words[idx]\n",
    "        c = weights[idx]\n",
    "        direction = \"POSITIVE\" if c > 0 else \"NEGATIVE\"\n",
    "        print(f\"{w:<20}  contribution: {c:.4f} ({direction})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c0816-4a81-44c2-a3e4-0d1ec8c31ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_word_influences(\"I thought the movie was dull and boring but the ending was great.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda084d-0d35-46d6-963d-f6979fd11e33",
   "metadata": {},
   "source": [
    "### 2.5 Evaluating the model\n",
    "To evaluate the model on our test dataset, we need to process transform the dataset with the same TF-IDF vectorizer we learned from the train dataset. \n",
    "\n",
    "We can then use the metrics functions from sklearn, as we have been doing in previous classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b653149-6352-46dd-a4c5-d5fbcf8ee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "# True labels\n",
    "y_test = df_test[\"label\"].values\n",
    "# Transform test split\n",
    "X_test_tfidf = tfidf.transform(df_test[\"text\"])\n",
    "\n",
    "# LogReg predictions\n",
    "y_pred_lr = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"LogReg confusion matrix:\\n\", cm_lr)\n",
    "print(\"\\nLogReg classification report:\\n\",\n",
    "      classification_report(y_test, y_pred_lr, target_names=[\"negative\", \"positive\"]))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm_lr,\n",
    "                       display_labels=['negative', 'positive']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062fa08-ea7c-4d3c-b39e-cf67cc3d74d0",
   "metadata": {},
   "source": [
    "## 3. Neural Network Classifier (Feedforward Neural Network)\n",
    "\n",
    "In this section we build a simple neural classifier for sentiment:\n",
    "\n",
    "**Text → tokens → embedding vectors → mean pooling → hidden layer → output layer**\n",
    "\n",
    "This model goes beyond logistic regression:\n",
    "\n",
    "- Logistic regression learns only a *linear* relationship.\n",
    "- Neural networks can learn *non-linear* patterns and interactions.\n",
    "- Embeddings allow the model to learn semantic similarities between words.\n",
    "\n",
    "We will:\n",
    "1. Tokenize the text.\n",
    "2. Build a vocabulary.\n",
    "3. Train an embedding + feedforward neural network using PyTorch.\n",
    "4. Compare its predictions to the logistic regression model qualitatively.\n",
    "\n",
    "Before that, let's make sure we have torch available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717bf95-15e6-4dcd-8310-608aab59601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a924b00-be8f-4277-8d3c-7ce884d3a76a",
   "metadata": {},
   "source": [
    "### 3.1. Preparing the data\n",
    "\n",
    "#### Step 1: Text → Tokens\n",
    "We start with raw text like:\n",
    "\n",
    "> \"The movie was fantastic and I loved every minute.\"\n",
    "\n",
    "We split it into **tokens** (words):\n",
    "\n",
    "`[\"the\", \"movie\", \"was\", \"fantastic\", \"and\", \"i\", \"loved\", \"every\", \"minute\"]`\n",
    "\n",
    "This is done with a simple tokenizer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23f347-80bb-4d8a-b64a-0dc7156f44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    # lowercase, strip punctuation\n",
    "    text = text.lower()\n",
    "    doc = nlp.make_doc(text)\n",
    "    return [tok.text for tok in doc if tok.is_alpha and not tok.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2ca6f-add0-4de4-af63-46e661852c2f",
   "metadata": {},
   "source": [
    "#### Step 2: Tokens → Indices\n",
    "The model does not understand strings.\n",
    "We build a **vocabulary** of all words in the training set:\n",
    "\n",
    "`word2idx = {'<pad>':0, '<unk>':1, 'the':2, 'movie':3, …, 'fantastic':1234}`\n",
    "\n",
    "This can later help us convert each token into an **integer**:\n",
    "\n",
    "`[2, 3, 10, 1234, 5, 7, 99, 22, 18]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20220ef3-57a3-4b52-9c9a-fa92fb249dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 1. Tokenize all texts in the training set\n",
    "# df_train[\"text\"] is a Series of strings.\n",
    "# We apply our tokenize() function to each text so that we get lists of tokens.\n",
    "tokenized_docs = df_train[\"text\"].apply(tokenize)\n",
    "\n",
    "# 2. Count how often each word appears in the training data\n",
    "word_counts = Counter()\n",
    "for tokens in tokenized_docs:\n",
    "    for word in tokens:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "print(\"Number of unique words before filtering:\", len(word_counts))\n",
    "\n",
    "# 3. Build the vocabulary list\n",
    "#    We add special tokens first, then all words that appear at least 5 times.\n",
    "vocab = []\n",
    "\n",
    "# Special tokens\n",
    "vocab.append(\"<pad>\")  # index 0\n",
    "vocab.append(\"<unk>\")  # index 1\n",
    "\n",
    "# Add words that are frequent enough\n",
    "min_freq = 5\n",
    "for word, count in word_counts.items():\n",
    "    if count >= min_freq:\n",
    "        vocab.append(word)\n",
    "\n",
    "print(\"Number of words in vocabulary (after filtering):\", len(vocab))\n",
    "\n",
    "# 4. Create the mapping: word -> index\n",
    "word2idx = {}\n",
    "\n",
    "for idx, word in enumerate(vocab):\n",
    "    word2idx[word] = idx\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Index of <pad>:\", word2idx[\"<pad>\"])\n",
    "print(\"Index of <unk>:\", word2idx[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc9171-591c-4328-8443-4766de369740",
   "metadata": {},
   "source": [
    "#### Step 3: Enconding the documents\n",
    "Documents (reviews) can be of varying length but the input of the neural network is fixed. So we need a way to fix the input to a specified size. What we can do is to set a length based on the document length distribution, and then either pad short documents or truncate long documents.\n",
    "\n",
    "##### Inspect the document length distribution\n",
    "We can set on a length that will cover `90-95%` of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529dae4-afd2-4d05-b04a-30b77981d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = [len(tokens) for tokens in tokenized_docs]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(doc_lengths, bins=50)\n",
    "plt.xlabel(\"Number of tokens per document\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of document lengths (train set)\")\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# we can have a look at the percentiels. \n",
    "for p in [50, 75, 90, 95, 99]:\n",
    "    value = np.percentile(doc_lengths, p)\n",
    "    print(f\"{p}th percentile: {value:.1f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20157d1c-5bf8-44b2-a3d6-7cb43667662c",
   "metadata": {},
   "source": [
    "##### Encode and pad documents for a fix doc size\n",
    "Here we encode the documents with the token indexes, padding or truncating the text based on the MAX_LEN that we chose. For example:\n",
    "\n",
    "`MAX_LEN=9`\n",
    "\n",
    "Example 1: \n",
    "\n",
    "`[\"the\", \"movie\", \"was\", \"fantastic\", \"and\", \"i\", \"loved\", \"every\", \"minute\"]`\n",
    "\n",
    "`encode + pad -> [2, 3, 10, 1234, 5, 7, 99, 22, 18]`\n",
    "\n",
    "Example 2:\n",
    "\n",
    "`[\"the\", \"movie\", \"was\", \"bad\"]`\n",
    "\n",
    "`encode + pad -> [2, 3, 10, 28, <pad>, <pad>, <pad>, <pad>, <pad>]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5196da-56e6-40a0-abe4-57c8babd4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "\n",
    "def encode(tokens):\n",
    "    \"\"\"\n",
    "    Convert a list of token strings into a list of integer IDs\n",
    "    using the word2idx vocabulary.\n",
    "\n",
    "    Unknown words are mapped to the <unk> index.\n",
    "    \"\"\"    \n",
    "    return [word2idx.get(tok, word2idx[\"<unk>\"]) for tok in tokens]\n",
    "\n",
    "def pad(seq):\n",
    "    \"\"\"\n",
    "    Make all sequences the same length (MAX_LEN).\n",
    "\n",
    "    - If seq is shorter than MAX_LEN: append <pad> tokens at the end.\n",
    "    - If seq is longer than MAX_LEN: truncate it to MAX_LEN.\n",
    "    \"\"\"    \n",
    "    if len(seq) < MAX_LEN:\n",
    "        return seq + [word2idx[\"<pad>\"]] * (MAX_LEN - len(seq))\n",
    "    return seq[:MAX_LEN]\n",
    "\n",
    "# We can test it\n",
    "tok_example = [\"the\", \"movie\", \"was\", \"fantastic\", \"and\", \"i\", \"loved\", \"every\", \"minute\"]\n",
    "tok_encoded = encode(tok_example)\n",
    "print(\"ENCODED -> \", tok_encoded)\n",
    "print(\"PADDED  -> \", pad(tok_encoded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabfe09f-f136-4243-8f7f-9c497ab25de5",
   "metadata": {},
   "source": [
    "#### Now we can encode our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e4460-2dd3-43bb-af6d-bbe56316c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ids = [pad(encode(toks)) for toks in tokenized_docs]\n",
    "y_train_ids = df_train[\"label\"].values\n",
    "\n",
    "\n",
    "X_test_ids = [pad(encode(tokenize(t))) for t in df_test[\"text\"]]\n",
    "y_test_ids = df_test[\"label\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07143f3-86c3-417d-894e-106038a78162",
   "metadata": {},
   "source": [
    "### 3.2 Wrapping our data in a `Dataset` (so PyTorch can use it)\n",
    "\n",
    "So far, we have:\n",
    "\n",
    "- `X_train_ids`: a list/array of input sequences  \n",
    "  – each element is a list of token IDs of length `MAX_LEN`  \n",
    "- `y_train_ids`: a list/array of labels  \n",
    "  – each element is `0` (negative) or `1` (positive)\n",
    "\n",
    "To train with PyTorch, we put this data into a small **wrapper class** that tells PyTorch two things:\n",
    "\n",
    "1. **How many examples** are in the dataset  \n",
    "   → this is what `__len__` returns  \n",
    "2. **How to get one example** (input and label) by index  \n",
    "   → this is what `__getitem__` returns  \n",
    "\n",
    "PyTorch has a standard interface for this called `torch.utils.data.Dataset`.\n",
    "We create our own subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217106e-2123-4bb3-9456-e09a52addafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Small wrapper around our data so that PyTorch knows:\n",
    "    - how many examples we have\n",
    "    - how to get one example (inputs + label) by index\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        # X: list/array of sequences (lists of token IDs)\n",
    "        # y: list/array of labels (0 or 1)\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of examples in the dataset.\n",
    "        This is used by DataLoader to know when an epoch is finished.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return one example (inputs, label) at position idx.\n",
    "        This is used when DataLoader creates batches.\n",
    "        \"\"\"\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SentimentDataset(X_train_ids, y_train_ids)\n",
    "print(\"Number of training examples:\", len(train_ds))\n",
    "print(\"Example 0 shapes:\", train_ds[0][0].shape, train_ds[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3637eb-da02-48b4-9d00-51948753aa17",
   "metadata": {},
   "source": [
    "### 3.3 Defining the Feed Forward Neural Network\n",
    "\n",
    "The neural network we use here follows the same structure introduced in the lecture:\n",
    "\n",
    "**Embedding → Mean Pooling → Hidden Layer (ReLU) → Output Layer**\n",
    "\n",
    "It takes a sequence of token IDs as input (one padded review per row) and converts it into a single prediction: *negative* or *positive*.\n",
    "\n",
    "Here is what each part of the model does:\n",
    "\n",
    "1. **Embedding layer**  \n",
    "   Turns each word index into a dense vector of size `embed_dim`.  \n",
    "   The embedding matrix has shape **[vocab_size × embed_dim]**.\n",
    "\n",
    "2. **Mean pooling**  \n",
    "   Reviews have different lengths after tokenization, so we average all word embeddings  \n",
    "   across the sequence to get **one fixed-size vector per document**.\n",
    "\n",
    "3. **Hidden layer + ReLU**  \n",
    "   A linear transformation followed by a non-linearity (ReLU).  \n",
    "   This is the part that allows the model to learn **non-linear decision boundaries**.\n",
    "\n",
    "4. **Output layer**  \n",
    "   Maps the hidden representation to **2 logits**, one for each class  \n",
    "   (negative/positive).  \n",
    "   A later softmax will convert these logits into probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34000626-cc9f-47b7-a93f-ae4fa4e9cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=64, num_classes=2):\n",
    "        \"\"\"\n",
    "        vocab_size: size of the vocabulary\n",
    "        embed_dim: dimensionality of word embeddings\n",
    "        hidden_dim: size of the hidden layer\n",
    "        num_classes: output classes (positive/negative = 2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Embedding layer: matrix of shape [vocab_size × embed_dim]\n",
    "        # Each word index maps to a trainable vector.\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, \n",
    "            embed_dim, \n",
    "            padding_idx=word2idx[\"<pad>\"]\n",
    "        )\n",
    "\n",
    "        # 2) Hidden layer: Linear transformation from embedding → hidden\n",
    "        # This is exactly W1 x + b1 in the slides.\n",
    "        self.hidden = nn.Linear(embed_dim, hidden_dim)\n",
    "\n",
    "        # 3) Non-linearity: ReLU\n",
    "        # This creates curved / non-linear decision boundaries.\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 4) Output layer: hidden_dim → num_classes (logits)\n",
    "        # This is W2 h + b2 in the slides.\n",
    "        self.output = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch, seq_len), containing token indices\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert indices to embeddings: (batch, seq_len, embed_dim)\n",
    "        emb = self.embedding(x)\n",
    "\n",
    "        # Mean pooling across the sequence dimension:\n",
    "        # (batch, embed_dim)\n",
    "        pooled = emb.mean(dim=1)\n",
    "\n",
    "        # Hidden layer + non-linearity\n",
    "        h = self.relu(self.hidden(pooled))\n",
    "\n",
    "        # Output layer: class scores (batch, num_classes)\n",
    "        out = self.output(h)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = FFNN(len(vocab)).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc513f-56db-4b5a-8977-4d21fe2c3783",
   "metadata": {},
   "source": [
    "### 3.4 Training the FFNN\n",
    "\n",
    "To train our neural network, we need to follow the steps seen in the lecture, involving the forward pass, loss computation and backpropagation. The learnign then occurs by minimising the loss over various passes over the training dataset (epochs).\n",
    "\n",
    "#### Defining the data loader\n",
    "The `DataLoader` is PyTorch’s mechanism for serving data to the model during training.  \n",
    "Instead of feeding all training examples at once, the DataLoader:\n",
    "\n",
    "- **splits the dataset into mini-batches** (here of size 32),  \n",
    "- **shuffles the data** at the start of each epoch to avoid learning order-based patterns, and \n",
    "- **delivers one batch at a time** inside the training loop.\n",
    "\n",
    "This batching is crucial: it makes training faster, and help stabilizing gradient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5bf63-6afd-46d8-af50-67c316958ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Wrap it in a DataLoader that:\n",
    "# - serves batches of size 32\n",
    "# - shuffles the data at the beginning of each epoch\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Let's inspect one batch\n",
    "for X_batch, y_batch in train_dl:\n",
    "    print(\"Batch X shape:\", X_batch.shape)  #(batch_size, MAX_LEN)\n",
    "    print(\"Batch y shape:\", y_batch.shape)  # (batch_size,)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1c4f7-fce7-42b6-a59b-93fdec8ec6b0",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "\n",
    "During training, we repeatedly show the model small batches of data and update its \n",
    "parameters using gradient descent.\n",
    "\n",
    "The key components are:\n",
    "\n",
    "- **Loss function (`CrossEntropyLoss`)**  \n",
    "  Measures how far the model’s predictions are from the true labels.  \n",
    "  In our case, the loss compares the predicted class scores (logits) with the correct class.\n",
    "\n",
    "- **Optimizer (`SGD` or `Adam`)**  \n",
    "  Applies gradient descent to update the model’s weights.  \n",
    "  Adam is a widely used variant of SGD that adapts the step size automatically, making\n",
    "  training more stable and faster.\n",
    "\n",
    "Inside each epoch, the following steps occur for every mini-batch:\n",
    "\n",
    "1. **Forward pass:** compute logits from the model.  \n",
    "2. **Loss computation:** evaluate how wrong the predictions are.  \n",
    "3. **Backward pass:** compute the gradients of the loss with respect to all trainable parameters.  \n",
    "4. **Optimizer step:** update the parameters in the direction that reduces the loss.  \n",
    "5. **Repeat** for all batches.\n",
    "\n",
    "After each epoch, we report the **average training loss**, which gives a sense of how well the model is fitting the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd69d2-e0d6-4496-8ba5-a0d3a4f27b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for X_batch, y_batch in train_dl:   # batches come from the DataLoader\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Forward pass\n",
    "        logits = model(X_batch)\n",
    "\n",
    "        # 2. Compute loss\n",
    "        loss = criterion(logits, y_batch)\n",
    "\n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Average loss = {total_loss/len(train_dl):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7088c17-395a-471c-9642-19264c0712d0",
   "metadata": {},
   "source": [
    "### 3.5 Predicting with our neural network\n",
    "Unlike scikit-learn, PyTorch models do **not** come with a built-in `predict` or `predict_proba` function.  \n",
    "We only have a `forward` pass (`model(x)`) that takes tensors and returns **logits** (raw scores for each class).\n",
    "\n",
    "To make this easier to use with text, we define a small helper function that:\n",
    "\n",
    "1. **Preprocesses the input text**  \n",
    "   - `tokenize(text)` → list of tokens  \n",
    "   - `encode(tokens)` → list of word IDs  \n",
    "   - `pad(ids)` → fixed-length sequence (`MAX_LEN`)  \n",
    "   - wrap into a tensor of shape `(1, MAX_LEN)` and move it to the right device\n",
    "\n",
    "2. **Runs the model to get logits**  \n",
    "   - `logits = model(x)` gives unnormalised scores for each class\n",
    "\n",
    "3. **Normalises the scores with softmax**  \n",
    "   - `softmax` turns logits into a probability distribution over the two classes  \n",
    "   - we use `F.softmax(logits, dim=1)` to get probabilities that sum to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e1cf6-356c-4bf2-9346-ba17ea2f26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenize(text)\n",
    "    ids = encode(tokens)\n",
    "    ids_padded = pad(ids)\n",
    "    x = torch.tensor([ids_padded], dtype=torch.long).to(device)  # shape: (1, MAX_LEN)\n",
    "    return x\n",
    "\n",
    "def predict_proba_nn(text):\n",
    "    model.eval()\n",
    "    x = preprocess_text(text)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)                       # shape (1, 2)\n",
    "        probs = F.softmax(logits, dim=1)[0]     # shape (2,)\n",
    "\n",
    "    # convert to python floats\n",
    "    return probs.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41606e0-3fa2-4859-9681-5615c34664ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict_proba_nn(\"I absolutely loved this movie!\")\n",
    "print(\"Negative:\", probs[0])\n",
    "print(\"Positive:\", probs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77e3df-7e87-4c92-9dea-7f8b264356c9",
   "metadata": {},
   "source": [
    "## 4. Evaluating the trained neural network\n",
    "\n",
    "After training, we want to know:\n",
    "\n",
    "**How well does the model do on unseen data?**  \n",
    "    For this, we use the **test set** (X_test_ids, y_test_ids).\n",
    "\n",
    "\n",
    "Testing on the test set has three steps:\n",
    "\n",
    "1. Put the model in **evaluation mode** with `model.eval()`.  \n",
    "   (This tells PyTorch we are not training anymore.)\n",
    "2. Loop over the test data, compute the model’s predictions, and compare them\n",
    "   to the true labels.\n",
    "3. Compute a simple metric, e.g. **accuracy** = number of correct predictions / total.\n",
    "\n",
    "We do all of this inside a `torch.no_grad()` block so that PyTorch:\n",
    "- does not track gradients, and\n",
    "- runs faster and uses less memory during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5104f-a540-4ccc-97fd-efd3dcc4100a",
   "metadata": {},
   "source": [
    "### 4.1 Evaluation loop\n",
    "We first wrap the dataset with the expected format for PyTorch (`SentimentDataset`), and create a DataLoader for the evaluation loop - same way we did for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56b9d4-e11d-4403-a93d-bf7aacbc7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset and dataloader (similar to train_ds/train_dl)\n",
    "test_ds = SentimentDataset(X_test_ids, y_test_ids)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False) # no need to shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7206eca-3baf-4fb1-9be1-f1a46dd1f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # set model to evaluation mode\n",
    "\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "with torch.no_grad():  # no gradients needed for testing\n",
    "    for X_batch, y_batch in test_dl:\n",
    "        X_batch = X_batch.to(device) # so that model and batch is in the same memory\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # 1. Forward pass: get logits\n",
    "        logits = model(X_batch)              # shape: (batch_size, num_classes)\n",
    "\n",
    "        # 2. Take the argmax to get predicted class index\n",
    "        preds = torch.argmax(logits, dim=1)  # shape: (batch_size,)\n",
    "\n",
    "        # 3. Store results as plain Python ints\n",
    "        all_y_true.extend(y_batch.cpu().numpy().tolist())\n",
    "        all_y_pred.extend(preds.cpu().numpy().tolist())\n",
    "\n",
    "len(all_y_true), len(all_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f58eda-2e13-4c9b-a6aa-0d23fa33f982",
   "metadata": {},
   "source": [
    "Note above that:\n",
    "- `.to(device)` → move tensors to where the model is (CPU/GPU) for computation.\n",
    "- `.cpu().numpy()` → move results back to CPU and convert them so libraries like NumPy / sklearn can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3214332b-fcad-4bde-8807-f58ff91e75c5",
   "metadata": {},
   "source": [
    "### 4.2 Computing metrics\n",
    "We apply again the metrics over the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8bfd15-c17a-44d9-ae9e-b40d463dd547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "print(classification_report(all_y_true, all_y_pred, target_names=[\"negative\", \"positive\"]))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                       display_labels=['negative', 'positive']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0865b3d-a395-4cb3-b6d9-a94fa0e48666",
   "metadata": {},
   "source": [
    "## 5. Further experiments and reflection\n",
    "\n",
    "1. **Vary the training set size**\n",
    "Train both models on:\n",
    "- 1,000 examples  \n",
    "- 5,000 examples  \n",
    "- 10,000 examples  \n",
    "- 20,000 examples  \n",
    "\n",
    "Questions:\n",
    "- Which model benefits more from additional data?  \n",
    "- At what point does the NN start outperforming TF–IDF + LogReg (if at all)?  \n",
    "- What happens to overfitting as data grows?\n",
    "\n",
    "2. **Change the Neural Network Architecture**\n",
    "Try adjusting:\n",
    "- Embedding dimension (50, 100, 200)\n",
    "- Hidden layer size (32, 64, 128)\n",
    "\n",
    "Questions:\n",
    "- Does a bigger network always help?  \n",
    "- When do you start to see signs of overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee23af4-3684-45d0-9f80-486f41072204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP Course",
   "language": "python",
   "name": "nlp-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
